% !TeX document-id = {55487e37-df6e-4d5b-af2f-b0275db5df41}
%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
\documentclass[acmlarge,screen]{acmart}
% !TeX TXS-program:compile = txs:///pdflatex/[--shell-escape]

%%%% Large double column format, used for TOG
%\documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{listings}
\usepackage{minted} 
\usepackage{subcaption}
\usepackage{float}
\usepackage{wrapfig}
\restylefloat{figure}
\usemintedstyle{friendly}
\graphicspath{ {../images/} }



%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{UVU MCS Graduate Paper}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Benjamin Stoneking}
\affiliation{%
 \institution{Candidate}
}
\email{benjamin.stoneking@protonmail.com}

\author{Frank Jones}
\affiliation{%
 \institution{Advisor}
}
\email{frankj@uvu.edu} 

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Candidate First Last}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
	The project explores the process of implementing an embedded real-time digital audio synthesizer. Emphasis is placed on leveraging the inherent strengths of the hardware (timers, hardware interrupts, etc) to produce a reliable musical instrument. The project is a focused exercise in implementing a digital audio processing pipeline with software implementations of common components such as oscillators, pitch control, signal gain attenuation, and filtering. It illustrates the process of implementing a capable, complex and extendable embedded system on a platform with limited processing power. The final product is a fundamental subtractive synthesizer: It responds to input from generic MIDI devices to produce an audio signal. It applies volume dynamics and timbre control to the signal using envelopes and filters. The device has a hardware interface consisting of knobs, encoders, and a screen allowing direct, real-time manipulation of the processing. A PC application can control the system remotely. The system is robust, but still lacks some additional core features. Implementing the expected functionality of electronic musical instruments was challenging. The project code base is close to 2250 lines. I spent substantial time just writing drivers to interface with hardware which often added up to very few lines of code. Processing speed and instruction space is limited and large code bases can quickly outgrow the capabilities of the device requiring efficient and concise code. Substantial effort and time was required to prototype the hardware interface on a breadboard and revise as new components were integrated. More time was required to design and build a permanent hardware fixture on a circuit board.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{audio, synthesis, embedded, microcontroller, real-time, midi, hardware, oscillator, filter, envelope, DSP}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\subsection{Purpose}
	The acquisition of musical gear is a common obsession that plagues millions of musicians around the world. The cost of gear for performance or music production presents a major financial struggle to musicians who already must work within a tight budget. A common thought occurs to musicians: "What if I just make my own X? Maybe I can save some money." This discussion occurs frequently within online and offline forums. Often times, musicians who have experience in making their own digital or analog instruments will give the same answer: "Yes. You can just make your own X. But no. You won't save any money. You will spend more and you should just save up some cash to buy your gear from a compony like Roland, Yamaha, Korg, etc." Over the years, I've researched this very topic and found this conclusion to be true but with a small caveat: You can in fact save money by making your audio production tool as a \textbf{digital system}. You can write as much software as you want. You can refactor and rebuild you software tool as much as you want, and never spend a dime. When physical electronic components are removed from the equation, if you have a system to run your software, you can make all your music production gear for free. This caveat has a caveat of its own: This takes substaintial time. Time is money. Therefore, every hour spent in implementing your system is implicitly adding to the price tag of your system.
	
	I've known these facts for a long time and have seen them in action as my education has had me walk through the process of making \textit{yet another} database management system, and \textit{yet another} virtual machine. Unsurprisingly, my database hasn't replaced MySQL and my virtual machine has not replaced the Java Virtual Machine. Reinventing the proverbial wheel did not make the effort fruitless. Valuable wisdom and knowledge of how computers work was gained from these experiences. I applied this same principle to my experience as a musician with the goal of my master's project: Make \textit{yet another} digital synthesizer on an embedded microcontroller. \textbf{The endgame of this project was the journey itself: to learn how to implement a complex real-time embedded system, develop a digital audio pipeline, make a hardware interface to control it and see what invaluable knowledge can be extracted from the experience.} There is also the intrinsic side benefit of indulging my personal fantasy of making a piece of musical gear that is wholly mine. The aim of this project was to accomplish the task of creating this device and documenting the process and lessons learned: What is a valid working architecture to making such a system? Where does one begin? What kind of failure and setbacks should be expected? Can it be done within a reasonable amount of time without substantial reliance on previously existing frameworks and libraries? What sort of mathematical and technical knowledge is required?

\subsection{Project Criteria}

	\subsubsection{Minimum Requirements}
	\label{requirements}
	In order to meet the definition of a hardware subtractive synthesizer it must meet the following minimum requirements (see Concepts for definitions):
	\begin{enumerate}
		\item The device must produce audio at a minimum quality of 44.1khz and 16 bit-depth (CD Audio Quality)
		\item Notes can be played by way of any generic MIDI keyboard.
		\item An \textbf{oscillator} component must be implemented. It must generate the most common wave forms: Sin, sawtooth, square, triangle and white noise
		\item The synthesizer (hereafter referred to as synth), but be polyphonic, capable of playing up to 8 discrete notes at a time.
		\item A \textbf{voice} component allowing mixing of multiple oscillator waveforms must be implemented.
		\item An \textbf{envelope} component must be implemented to dynamically attenuate the gain of each voice over time.
		\item A \textbf{filter} component must be implemented to be able to selectively cut frequencies from the final signal. It should support lowpass, highpass and bandpass filtering. Filter frequency cutoff and resonance must be adjustable in real-time.
%		\item A \textbf{low-frequency oscillator (LFO)} component must be implemented which the user may configure to modulate arbitrary signals within the system.
%		\item Custom "patch" saving. Users can save their current configuration to a patch bank, and recall their saved a previous.
		\item A \textbf{hardware interface} allowing personal interaction between the user and the system must be implemented. A MIDI input jack must be included for plugging in a keyboard. A 1/8" audio output jack should allow the user to hear playback with headphones or speakers. It should have a series of knobs and a screen communicate the system state to the user. The knobs should allow the user to navigate a menu to select a variety of parameters they wish to manipulate. The knobs should adapt to the context of the menu to adjust parameters appropriately.
	\end{enumerate} \cite{rise_2016}

%	\subsubsection{Additional Features}
%	Mentioned above is the absolute bare minimum representation of what would constitute a common subtractive synthesizer. A series of other common but not required features were planned for the project:
%	\begin{itemize}
%		\item Additional generic envelope for modulating arbitrary signals on system
%		\item MIDI Pitch and Modulation wheel support
%		\item Additional audio FX: Delay, reverb, overdrive, etc.
%		\item 3D printed enclosure box
%		\item Expand synthesis options: Frequency Modulation (FM Synthesis), Sample based synthesis
%	\end{itemize}
	
	\subsubsection{Constraints}
	To maximize the learning potential of this project and focus on the process rather than the final destination, I decided to set a few restrictions:
	\begin{enumerate}
		\item Language: The firmware must be implemented in a low-level programming language such as C/C++.
		\item Libraries: Only the standard C/C++ libraries may be used. A Hardware Abstraction Layer to minimize hardware peripheral set up time may be used. Other than that exception, all software must be written from scratch. This includes digital signal processing (DSP) libraries; However, every audio processing function is handcrafted.
		\item References: Referencing and reading other implementations is not explicitly restricted. Every effort must be put forth to understand the mathematical concepts of DSP and implement them without reliance on existing implementations.
	\end{enumerate}
	
\subsection{Results}
	The final state of project met most of the minimum requirements but failed to fulfill every feature: There was not enough time to implement LFOs info the system, and HAL library issues involving writing and reading from flash memory space got in the way of fully implementing a user patch bank. The system is able to perform its function as a versatile musical instrument. It is relatively stable, but needs a few revisions before it can be a reliable studio or stage performance-ready instrument.
	
	\subsubsection{What was learned?}
	
	\paragraph{The device platform must be chosen carefully} I chose the audio development board Daisy by Electro-Smith. It is marketed as an attractive option for musicians to implement their dream systems because of its high resolution digital to analog (DAC) converter and painless hardware peripheral abstractions. It turned out to not be as mature and stable as I hoped and had many issues with drivers, pin connections and lacking support for certain interrupt service routines. At the time, it was difficult to see that Daisy may not have been the best option for implementing system scoped beyond a hobbyist project. The platform was designed for musicians to implement simpler systems with highly abstracted development libraries and programming languages/frameworks like MaxMSP, Puredata and Arduino. It appears more attention and care was put into supporting these tools and making them work well while some lower level capabilities were sidelined. The Daisy platform is very popular and is used commercially but still has yet to reach a highly stable and mature state.
	
	\paragraph{Hardware prototyping must be incremental} After I had spent several months setting up my hardware peripherals on a breadboard and implementing core functionality, I felt the need to migrate the system off a breadboard to a more permanent and stable home. It was at this point I tried to do too much at once. I tried to design a soldered circuit board able to be embedded in a hardware enclosure that was yet to be designed. My thought was that it may take several revisions of the enclosure before I knew where all my knobs, encoders, inputs, outputs and screen needed to be for ease of use and optimal functionality. I built up flying wire harnesses for all peripherals so that they could be moved around to test out different enclosure layouts. The result was an absolute mess of wires that were susceptible to interference. My first step in taking the system off the breadboard should have been simply solder my components into logical sections on perf board and make the system work as it did on the breadboard. It would not have been optimal or have fit into an easy enclosure. I would still have to design a final enclosure and migrate the system to its final home, but I would have saved so much time soldering and troubleshooting my sloppy design. I could have allocated more time to software design and would have been able to complete more features like the LFO and patch bank mechanic.
	
	\paragraph{Pure software functions should be decoupled from hardware and tested independent of the hardware} My workflow for the entirety of the project was \textbf{write code, compile, flash, test, repeat}. This involves a lot of waiting. Waiting for the compiler. Waiting for the bootloader. Waiting for the flash process to complete. Part of me understood that something could be improved. I should have designed my code more carefully to not be so tightly coupled to the Daisy system. I could have wrote some pure software test fixture code that runs on my host PC to test my different synth components independently. That would require me to spend less time waiting and more time writing and testing my software incrementally. \textit{Example:} for my oscillator code, I could have written an oscillator test fixture that used my oscillator code to generate the fundamental waveforms and compare them to real world samples to ensure that my code was producing satisfactory results. After the tests passed, I could \textit{then} integrate it into my firmware. I was concerned that if I wasn't testing on the hardware itself that I potentially miss out on capturing critical bugs that on the hardware. What if my instructions are too slow for the system? What if the component is too difficult to incorporate into the firmware? These were valid concerns but turned out be non issues. Some features were not completed because of unnecessary waiting and slow development time.
	
	\paragraph{Purchase hardware components from trusted sources} I made the mistake of sourcing some components from Amazon. The cheap price and quick delivery was too much of a temptation. Several days of agonizing troubleshooting and software debugging were wasted because of one encoder in a bag of ten. It happened to have been soldered to its PCB in reverse and it was completely defective. I assumed my code was the problem. Only after hooking the offending part up to a multimeter was I able to see the problem. I replaced the part and my system worked without a single change to the software.
	
	\subsubsection{Complexity}
	\paragraph{Lines of Code} The capstone project guideliens uses lines of code to quantify the complexity of the project. The project is expected to be around 5000 lines of code. Embedded systems have limited memory and speed requiring the programmer to design their algorithms and data structures to be concise and work as efficiently as possible. If the programmer does not operate in this way, the compiled firmware may be too big to be contained in the flash space. Additionally there may not be enough memory available to load the firmware from flash \textbf{and} contain the necessary stack and heap memory. In addition, adding features like the LCD screen required me to write driver code to display data in an efficient way. Over a week of time was invested just in writing the I2C driver code to make the LCD screen compatible with the system. The final driver code amounts to about 150 lines of code which is not representative of time and effort put in to implementing the functionality.
	
%	\paragraph{The monolithic code structure} The final form of the system can be described as complex if not complicated. I tried my best to separate my code into a \textbf{Model View Control (MVC)} pattern. The \textit{model} in this case is the audio synthesis structure of the firmware: the oscillators, voices, filters, parameter structure and the synth class that contains them. It represents the core functionality of the system. The \textit{view} portion of the MVC consists of the components that convey the state of the system to the user: the LCD screen, the menus and their submenus. The \textit{control} portion of the MVC is the way in which the user interacts with the system: all the physical knobs, inputs, MIDI events, etc. Keeping these parts of the MVC separate was difficult. I tried to keep these components loosely coupled and reduce occurence of sharing references to each other. In the end, some structure needed to own them. Hardware needed to receive inputs and communicate them to the Model and View. So in the end, the hardware layer, the \textit{control} in MVC, became the sole owner of everything.
	
\subsection{Outline of this Paper}
	\paragraph{The remainder of this paper will focus on a few core topics}
	\begin{enumerate}
		\item A concise glossary of terms to bring the reader up to speed on the key components of audio synthesis
		\item A section outlining how any referenced works relate to the project
		\item A complete description of the architecture of the project with an account of the most note-worthy steps in the implementation process. 
		\item A section summarizing the project as a whole: the successes, the failures, the laughter, the tears and the path forward for the project.
		\item A section expressing gratitude for the long-suffering support of my wife and children as well as faculty and a preachy commitment to the open source community and hacker ethic.
		\item A bibliography of references works and resources
	\end{enumerate}

\section{Related Works}
For each reference in your paper, write a two- or three-sentence paragraph 
describing how it relates to your work. Think of this as an annotated bibliography
covering your literature search in an organized progression that highlights your 
how your work is similar and different. This is particularly important if your project repeats
previous work. At least 15 sources is a reasonable number.

\textbf{Some authors prefer to include related works as part of the Introduction, because there
is overlap. For project defense purposes, it best to have it as a separate section.}

\section{Concepts}
	\textbf{Oscillator:} Generally speaking, an oscillator is a function generator. It will generate a repeating signal at a specified frequency much like the function generator of an oscilloscope.

\section{Software Architecture and Implementation}

\subsection{Main Section}
	\paragraph{The system is built as an interrupt driven pipeline architecture} A class called synth comprises the monolithic audio data processing pipeline. Two interrupt service routines function as the main operators of this pipeline: A \textbf{hardware timer} interrupt is responsible for capturing events from the user: moving knobs, navigating menus and playing notes on the keyboard. An \textbf{audio} interrupt, triggered by the high resolution DAC is responsible for getting audio output samples from the synth and pushing them to DAC's audio out buffer.
	
	\paragraph{The architecture can be describe as having five participants:} The \textbf{user} operating the synth and keyboard, the \textbf{menu screen} communicating the system state to the user, the \textbf{synth} generating audio samples, the \textbf{timer interrupt} reading hardware inputs to trigger events, and the \textbf{audio interrupt} requesting the next sample to write to the audio output DAC. Following is a series of sequence diagrams to describe common events within the system.
	
	\begin{figure}[H]
		\centering
		\caption{This sequence is the most common event for the runtime of the system. It represents the idle state when the user is not engaged in the system. The process audio callback is continuously requesting samples from the synth, and the synth is just returning a value of zero. The result is silence for the user. \textit{Note: although the timer appears in active, the interrupt is still occuring with no inputs to detect to trigger events.}}
		\includegraphics[height=4cm]{no_note_sequence}
	\end{figure}

	\begin{figure}[H]
		\centering
		\caption{The second most common event is the act of the user playing a note on the keyboard. The timer interrupt detects the MIDI message from the keyboard and triggers a \textit{note on} event for the synth. The synth then activates one of its voices and sets the voice's pitch according to the note on event. Next time the audio interrupt occurs, the synth will generate and return an audio sample from that voice. The audio interrupt puts the sample into the DAC buffer which plays out the speakers for the user. Audio will continue to generated and played to the user until the key is released from the keyboard. The timer triggers a \textit{note off} event, telling the synth to deactivate the voice. Next time the audio interrupt occurs, the synth will return a zero valued sampled which results in silence for the user}
		\includegraphics[width=.9\linewidth]{play_note_sequence}
	\end{figure}
	
	Keeping these actors in mind, we can now begin to understand how the synth, timer interrupt and audio interrupt work together.
	
	\subsubsection{The Synth}
	\paragraph{The synth component is a pipeline by design, so it is comprised of a series of modular components} In order to understand how it works, I will break it down into a series of its fundamental modular components that make up the pipeline. This section assumes some background knowledge of the components. For a brief explanation, see Concepts.
	
	\textbf{Oscillators}
	\paragraph{The pipeline must begin with the oscillator so that there is audio data to be processed by subsequent components} The oscillator component is a class with only a few attributes: An enum representing what kind of function it generates, a variable to track the phase of the function cycle, and delta variable for how much the phase should be incremented to achieve the desired frequency. Every time an audio interrupt occurs, the get\_sample method is called: The sample is generated by passing the phase variable to the function input. The oscillator's phase variable is incremented by the delta variable. If the oscillators phase exceeds the value of \( 2\pi \), the functions phase is complete and the oscillator will subtract \( 2\pi \) from phase variable to start the phase over. The generated sample is returned for the audio interrupt to push to the DAC's output buffer for playback. Below is a sequence chart illustrating this process with the oscillator generating a sin wave function.\cite{farnell_2010}
	
	\begin{wrapfigure}{l}{.75\textwidth}
		\includegraphics[width=\linewidth]{oscillator_sequence_diagram}
		\caption{Oscillator sequence diagram needs to be exported}
		\centering
	\end{wrapfigure}
	
	\paragraph{The phase and delta variables:} Mentioned previously, the delta (hereafter called phase\_delta to reference the code snippets) is a calculated value by which we increment the phase to get the next sample of the waveform for the desired pitch. So how on earth do we calculate this magic phase\_delta? Lets start simple, to generate a sin wave at 1hz with a sample rate of 44.1kz, we simply need to divide \( 2\pi \) by 44100 to get the phase delta. We start by initializing phase to zero and set phase\_delta to \( 2\pi/44100 \). Every time the audio callback occurs, the oscillator will return the value of \( sin(phase) \) and then increment phase by phase\_delta. After one second of time has passed, the audio callback will have triggered 44,100 times, phase\_delta will have been added to phase 44,100 times and phase will be equal to \( 2\pi \) completing one cycle, and a 1hz sin wave will have been played out the speaker. Okay well cool, but humans can't hear a 1hz sin wave and music is most enjoyable when it's audible. Let's make some noise we can hear: To change the pitch, we only need to multiply the 1hz phase\_delta by our desired frequency, so the formula for calculating the phase delta of your desired pitch is \( 2\pi/44100 * frequency \). \textit{Note: 44100 is not a hard and fast number. If you wish to have your DAC use a higher sample rate like 96khz, then 44100 would be substituted with 96000. So the generic formula is \( 2\pi/sample_rate * frequency \). The sample rate of the DAC and the sample rate used to calculate the phase\_delta \textbf{must} match or the pitch will be off} Okay, so going back to making music: If we want our oscillator pitch to be 100hz, our calculated phase\_delta will be \( 2\pi/44100 * 100.0\). This phase delta will cause the oscillator function to cycle 100 times in a second. Pretty simple. All this behavior is contained in the set\_pitch method in the Oscillator class.
	
	\paragraph{"What about other waveforms? Sin waves are boring!"} Totally. Sin waves lack harmonic complexity and will generally produce very smooth and mellow tones. Lets talk about that function enum I mentioned before. The Oscillator class has an enum called WaveForm to indicate what \textit{kind} of oscillator it is (i.e. what function does it use). The oscillator WaveForm enum is set with the set\_waveform method. Then the next time get\_sample is called by the audio callback, the function matching the oscillator's configured WaveForm will be used to generate a sample from your fancy new harmonically rich waveform. \cite{downey_2016} Beautiful! So what kind of functions are we using here? Besides the sin wave, all the classic wave forms are present: Sawtooth, Square, Triangle and White Noise. The square wave just an on/off function where half the time the returned value is -1.0 and the rest of the time the returned value is 1.0. The sawtooth save is a function that ramps gradually from low to high. The triangle resembles a sin wave but with a linear form instead of a curve. White Noise is generated with a pseudo-random number generator. \cite{tagi_2019} The snippet below is a good start for understanding the oscillators different waveform functions. See Oscillator.cpp for the full source.
	
	
	\begin{minted}{c}
		//Oscillator.cpp
		float Oscillator::get_sample()
		{
			switch (_waveform)
			{
				//Plain old sin wave. Simple and easy to generate
				case WaveForm::Sin: {
					sample = sinf(_phase);
					break;
				}
				//A triangle wave!. It's like a sin wave but pointy! That's neat.
				case WaveForm::Tri: {
					t   = -1.0f + (2.0f * _phase * TWO_PI_RECIP);
					sample = 2.0f * (fabsf(t) - 0.5f);
					break;
				}
				//Sawtooth. Sharp and full of harmonics that make it fat and sassy
				case WaveForm::Saw: {
					sample = ((_phase * TWO_PI_RECIP * 2.0f) * -1.0f) * -1.0f;
					break;
				}
				//The good old square wave. The classic sound of the 8-bit video game era
				//Chock full of vitamins and harmonic flavor!
				case WaveForm::Square: {
					sample = (_phase < _half_cycle) ? 1.0f: -1.0f;
					break;
				}
				//TV static. For noisy instruments like drums and sound effects
				case WaveForm::WhiteNoise: {
					sample = noise.process();
					break;
				}
			}
		}
	\end{minted}

	\paragraph{\textbf{Nyquist's theory} must be taken into account when dealing with oscillators} Nyquists theory states that a signal must be sampled more than twice its highest potential frequency to be accurately represented. When this requirement is not met, \textbf{aliasing} will occur on the undersampled frequencies. The highest frequencies perceivable to human ears is 22khz, which is why the industry standard sample rate is 44.1khz. This ensures that the highest signal frequencies within human hearing are accurately represented.

\subsection{Hardware Platform}
	\paragraph{A big decision in implementing this project was choosing the right microcontroller platform} I considered several options but settled on a popular and relatively new audio development ecosystem by Electro-Smith known as Daisy. Daisy is an Arm Cortex-M7 based development board that provides all the necessary processing power and peripherals necessary for audio processing. Daisy's core processor clocks at 480Mz and includeds extremely high-resolution digital-to-analog converter (DAC). In addition to the DAC, it supports the expected communication protocols such as I2C, UART, USB, Timers, etc. The biggest benefit to choosing Daisy as my platform was Electro-Smith's libDaisy library. libDaisy acts as an abstraction on top of the standard HAL libraries for the Cortex M7 and is designed to relieve the user of complications with hardware set up so they can focus on audio development. \cite{electro-smith} This made Daisy an attractive option. Audio processing was a completely foreign domain in programming, and Daisy coupled with its easy to use HAL seemed like a promising option that would give me the time I need to learn the ropes of audio development.
	
	\begin{wrapfigure}{r}{.5\textwidth}
		\includegraphics[width=\linewidth]{daisy_promo_pic}
		\caption{Daisy Seed by Electro-Smith. The microcontroller platform for this project}
		\centering
	\end{wrapfigure}

	\paragraph{Daisy seemed to be a great choice at the time} Within minutes of connecting Daisy to my machine, I was able to set up an audio callback. I used a random number generator to play white noise out the DAC. After a few minutes of soldering some wires to an audio jack and connecting them to the DAC pins, I could hear static playing out my headphones. Hello world! It was exhilarating to write only a few lines of code and hear it come to life after a few minutes. I felt very strongly that Daisy was the right choice. What I was not aware of was the relative newness of the platform. Daisy originally was a Kickstarter-funded project which received incredible amounts of success and support. The platform was designed with musicians in mind, and the developers put a lot of work into supporting much more user-friendly development languages and frameworks like MaxMSP, Puredata, and Arduino. This made the system highly accessible to musicians with little to know experience in audio development. These layers of abstraction seems to have some unintended side effects. It meant that a lot of time put into customer accessibility, and full support for utilizing all hardware to its fullest capabilities was put off and is still in current development.

%	\paragraph{This decision had unexpected consequences for my project} Some HAL library functions were not adaquately tested and did not work as they were supposed to.
%
%	\begin{enumerate}
%	\item One of the biggest disappointments was a lack of support for two-way USB communication outside of the bootloader. Toward the end of my project, I implemented a serial interfacing GUI application for remote control of my system. I discovered that the USB software support extended to either a dedicated interface for the device to either receive or transmit data over USB but not both. I had a choice to either hack a software solution involving handing off control between the host computer and device, or fix the USB configuration in the libDaisy library myself. A quicker option was to simply add a USB-UART converter chip and connect it to some generic UART pins on the device. I opted for the last option as I did not have extra time to waste at the end of my project.
%	\item Another severely lacking feature was support for hardware s. Some of the HAL libraries provided satisfactory support for hardware interrupts for USB receive (this was insultingly frustrating as I could not talk back on the USB line). Additionally, there was no support for UART and GPIO interrupt service routines (ISR). This is the reason why MIDI data events processed with hardware timer driven polling. I believe this polling method resulted in MIDI data consumption that was not completely reliable.
%	\item A last gripe I have with the platform is the datasheet claiming the availability of certain I2C and UART lines on the board's pins. It turned out that they are not all completely wired up in the board's current revision. This was not officially documented and instead made known through obscure forum posts on community forums. This lack of transparency and hardware support made hardware prototyping rather difficult.
%	\end{enumerate}
%
%	This whole experience soured me a bit about the Daisy platform in general. While I admire the ambition of the project to support programming in a variety of accessible languages that opens up audio development to new developers, I feel it would have been best to fully flesh out system in plain C before moving on to supporting things like Object Oriented Programming, visual programming languages, and Arduino. I \textbf{do} however relate to these developers biting off more than they could chew in their enthusiasm to implement these features. I made this same mistake in this very project (more on that later).

	\paragraph{My original vision for the project was, as the master's capstone prompt recommends, very ambitious} My goal was to implement a near clone of \textit{my} first synthesizer, the Korg microKorg. The microKorg is a small but very powerful analog synthesizer with a minimalist interface. It has a small array of buttons for selecting patches from bank, an array of multifunctional potentiometers and a small 3 digit 7 segment display. My project would be in a similar spirit but with a few minor change in hardware layout: it would not include a built in keyboard but would be a headless unit that could be used by plugging in any standard MIDI keyboard. It would include a more sophisticated display that could communicate more explicitly to the user. The microKorg is an industry standard powerhouse synth packed into a small and inexpensive form factor. My goal was ambitious to say the least. \cite{ward_2003} At one point, I did have a stretch goal in mind. If I finished all the audio components and fx, I would add a small array of buttons that would allow the user play some notes without needing to connect a keyboard. This can be seen in my original design drawing. I truly was a bright-eyed summer child when I first envisioned this project.

%	I will explain my implementation by first covering what I managed to complete. I will discuss the key components in the order they were implemented, and then have a small section cover the lesser features and a run down of the things I failed to complete.

\subsection{The Hardware Prototype}
	\paragraph{This section covers the hardware prototyping phase} It's important to cover hardware right out the gate. Much of the hardware was prototyped in parallel with the software because the system required the hardware interface in order to facilitate thorough testing.

	\subsubsection{MIDI}
	\paragraph{There is a 5-pin MIDI input circuit that allows the user to connect a standard MIDI cable between the MIDI output of a keyboard and the synthesizer} This functionality is plug-and-play. By plugging in the MIDI cable and powering on the device, the user can immediately start playing notes and music will play out headphones or a speaker  plugged into the 1/8" audio jack.
	
	\begin{wrapfigure}{l}{.5\textwidth}
		\includegraphics[width=8cm]{midi_optocoupler_circuit}
		\caption{The MIDI input optocoupler circuit}
		\centering
	\end{wrapfigure}

	\textbf{The MIDI circuit implements a standard of device safety for receiving incoming signals:} A device called an optocoupler (6N138) is the main player in the circuit. One side of the optocoupler receives incoming electrical signals from the MIDI input and converts it into an optical signal with an internal flashing diode. The other side senses the optical signal and converts it to an electrical signal that is safe for the device to read. This safety precaution is meant to handle cases where the connected keyboard is malfunctioning or outputting an electrical signal that is too high voltage for the pins on the MCU. If device failure occurs and the system is exposed to dangerous currents, the optocoupler will fry and the rest of the system remains protected. \cite{notesandvolts}
	
	\paragraph{After the oscillators were properly implemented, I added the MIDI message processing} I looked through some blog posts about consuming MIDI data through a UART connection and found the optocoupler circuit. I ordered the parts and got it working quick and easy. As data arrived on the UART line, I parsed the bytes into MIDI messages. MIDI note\_on messages were received when a note was pressed on the keyboard. MIDI note\_off messages were received when the key was released. Both message types contain a 7 bit number indicating the key that was pressed and a 7 bit number indicating the velocity (\textit{how hard} the key was pressed).\cite{huber_2007} When note\_on messages arrived, I converted the integers to the correct floating point frequency values and set the oscillators pitch. The oscillator would play the new pitch out the speakers. When note\_off messages arrived, the pitch of the oscillator was set to 0hz to silence the oscillator. At this point, I had the absolute bare minimum definition of a synthesizer. It could play a note sent from the keyboard, and the oscillator would respond by playing the correct pitch with its configured waveform. It could only play one note at a time and could not generate any signals more complex than the 5 basic oscillator functions, but nevertheless, it was a big step in the right direction. Adding polyphony and waveform mixing was the next logical step.	
	
	
	\subsubsection{Analog Potentiometers, Digital Rotary Encoder and Screen:}
	I ran with the design philosophy of the Korg microKorg and set up an array of 5 analog potentiometers. The potentiometers have control over various synth parameters. The parameters they effect are contextual, determined by the current internal state of the synthesizer. The internal state of the synthesizer is communicated to the user with a 20x4 character LCD screen that shows the user the current mutable context of the system. There is a main menu that displays a list of all the available menu context they can operate on. They can scroll through the main menu by turning the digital rotary encoder. The menu indicate which menu context is ready to be selected with an arrow symbol "<-". The user can then select the menu context by pushing down on the rotary encoder. They menu context will change to display the mutable parameters and the potentiometers will be mapped to the appropriate parameter values. Turning the turning the encoder knob will return them to the main menu and deactivate the potentiometers.


	\begin{figure}[H]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\caption{The Main menu screen. User navigates by through options using the rotary encoder. No menu context is currently selected making potentiometers inactive}
			\includegraphics[width=.9\linewidth]{complete_synth_pics/menu_navigation}
			\label{fig:sub1}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\caption{The oscillator context. Knobs 1-4 are mapped to the displayed parameters}
			\includegraphics[width=.9\linewidth]{complete_synth_pics/oscillator_context_small}
			\label{fig:sub2}
		\end{subfigure}
		\caption{Navigating the menus with the encoder and using the adaptive potentiometers to mutate system parameters}
		\label{fig:test}
	\end{figure}	

	For example, if the menu is displaying the oscillator control context, you can use the potentiometers to configure the oscillator waveforms and adjust pitch offset. If the menu was instead displaying the Amp Envelope control context, the knobs would be mapped to adjust the Attack, Decay, Sustain or Release parameters of the envelope. The parameter mapping also perform specific interpolations on the raw data values read from the potentiometers. Since some parameters are better controlled logarithmically, the integer value read from the knob can be converted to a float and scaled according to its required range. Some parameters only have a few discrete values. In these cases, the large values of the 16-bit numbers are instead mapped to the smaller ranges required to represent the discrete values of the parameter.

	\subsubsection{The rest of the hardware:}
	\textbf{Power:}
	The 20x4 LCD screen and the optocoupler on the MIDI input circuit require a 5v power supply, which Daisy does provide Yet another frustrating aspect of the Daisy platform since there is 5v power available on the USB jack. So I added a standard breadboard power supply to the circuit. It accepts a barrel jack input with voltages between 5v and 12v and steps the power down to 5v. This voltage is fed into the VIN pin on Daisy and is used to power the entire circuit and all its peripherals. A 1/8" inch stereo audio jack is connected to the left and right DAC pins on the MCU and outputs audio at line level gain out of the box without the need of an amplifier (a very nice feature). Finally, there is a USB to UART converter breakout board connected to a set of RX/TX UART pins on the MCU which allows two-way communication between an external computer and the synthesizer via a remote control GUI I wrote in Python.
	
	\textbf{Digital to Analog Converter (DAC):} 
	Daisy has a built-in high-resolution digital-to-analog converter. It is capable of outputting audio samples at rates up to 96khz and 24-bit depth. This resolution far exceedes the industry standard quality of 44.1khz and 16-bit depth for CDs. There are four DAC channels: two for stereo input and two for stereo output. In this project, I only use the stereo output as processing audio input is beyond the scope of this project.

	\textbf{UART, I2C, and 16-bit input ADC:}
	General purpose pins were configured to set up UART, I2C, and ADC communications Two separate UART lines were used for setting up MIDI input and plain serial communication for controlling the system remotely. The ADCs were used for reading the potentiometers. Some GPIO pins were used to read the digital rotary encoder for menu navigation. An I2C line is used to display characters to the LCD screen.

	\begin{figure}[H]
		\includegraphics[width=.8\linewidth]{complete_synth_pics/keyboard_and_synth_labeled}
		\caption{The entire synth hardware set up with labels}
		\centering
	\end{figure}

\subsection{Software Architecture}
	



\subsection{Pipeline Components}
	\paragraph{This section goes into depth about the various software components that make up the audio processing pipeline} I will address the software components in the order that they were implemented throughout the project. This will help to highlight the process of generating audio from the initial keyboard input to eventual output of the audio to a speaker. I will cover the topics of \textbf{Oscillators, Voices, Envelopes, and Filters} in depth as these are the critical components of the actual synthesis aspects of the project. I will only very briefly discuss other features in a few sentences as they have less to do with audio synthesis and more to do with overall system cohesion and functionality. \textit{See concepts for definitions and background info}

	

	\subsubsection{Voices}
	\paragraph{In the world of synthesizers, a single discrete note playing a certain pitch is called a voice} When a synth is \textbf{monophonic}, it means it only has one voice and can therefore only play one pitch at a time. Conversely, a \textbf{polyphonic} synth can play two or more discrete notes at a time. Most modern synthesizers, whether analog or digital, have polyphonic capabilities. Many analog synthesizers can only play four to eight notes at once because adding more voices requires adding more discrete hardware oscillators. Adding more voices to an analog system makes it more complex, expensive, and heavy. Digital synthesizers can accommodate more notes quite easily and will often support at least eight voices with options to play as many as 128 (the maximum number of notes available in the MIDI 1.0 protocol) which is overkill. I did a little bit of playing on a synthesizer of my own and I paid attention to how many notes I needed at my most advanced and complex playing. I decided that eight would suffice with the option to add more if needed.
	
	\paragraph{I wrote a class contain the functionality of a voice} The class would contain two separate oscillator class instances that can be set to any fundamental waveform. Each oscillator would also have a separate gain parameter to mix up some more complex hybrid waveforms. At the end of the processing chain for the voice, the oscillator samples would be summed into one sample and passed to the class's output. The voice class can optionally configure a pitch offset to the second oscillator; another common synth feature in synthesizers. By detuning one oscillator from another, you can generate even more complex waveforms with frequency beating, added intensity, or even complete dissonance if desired.

	\paragraph{Once the voice class was implemented, I wrote a larger class called Synthesizer that would own a group of voice instances} Each voice would start with 0hz for its pitch to indicate it was currently inactive and available to play a note. As keys on the keyboard were pressed, the synthesizer would find an inactive voice and activate it by setting the pitch to that note. Some optimization was made to ensure that if a voice wasn't in use, the synth class wouldn't bother asking the voice to generate a sample and waste time calculating a sample for 0hz.
	
	\begin{figure}[H]
		\includegraphics[width=\linewidth]{voice_graph_diagram}
		\caption{At this point in the system, the software architecture looks like this:}
		\centering
	\end{figure}
	
%	\paragraph{Implementing this voice collection opened a whole can of worms} What data structure should be used to hold these voices? There are many considerations to be made. But first, I was just focused on making it work. For this project, I decided to avoid any C++ features other than simple classes and subclassing, as there would be performance and memory usage overhead prices to pay that would add up over time. I started by using a primitive array. My approach was relatively simple. As a new note was played, the Synth class would iterate through the voice array until it found an inactive voice (pitch set to 0hz), and activate it by setting its pitch to the new note. When note-off events came in, the synth would iterate over the array until the voice playing the pitch matching the note-off event is found. The voice would then be deactivated by setting the pitch to 0. That was pretty easy. This was even quick to process. Most of the time you wouldn't have to iterate close to the end of the array and the time domain in which humans input notes is so much slower than the speed that the MCU is running at. So overwhelming the MCU with MIDI event processing just wasn't possible
%
%	\paragraph{The real rub was to handle when all voices were active and more note-on events occurred} Should the synth just ignore that note, or should it drop some other note and replace it with the newest one? I have observed that most synths seem to employ a policy of dropping the pitch that has been played the longest. That voice is then used to play the new incoming note. I decided to follow this policy. So how do I indicate the oldest note in the sequence?  My first approach to this was a poor choice. It involved a rather clunky method of making sure the voices in the array were sorted according to the order in which the pitches were played on the keyboard. Every time a note-off event occurred, I would have to shuffle voices around to make them preserve the order in which played their current pitch. In hindsight, this approach was so bad, it's mind-boggling. I never experienced any breakdown in the audio output because of the time spent sorting, but eventually, cracks started to appear when many notes were being played and released in quick succession. Sometimes, the sorting action would take long enough that midi events were missed on the UART bus, or some bug in my logic resulted voices failed to be turned off. When that happens, the voice will just play forever until the note that was stuck pressed and released again to give the synth a chance to turn it off.

%	\paragraph{In the end, I reverted my voice organization to simply iterating through the array to turn voices on and off} For now, in this relatively rare event when too many notes were being played, it just ignores new notes. The contrived sorting strategy was dropped and the system instantly became more stable. Upon reflection, I suspect their is an easy solution to this problem. Perhaps the synth could have a data member like a uint64 called note\_count. note\_count would count how many note on events occured since startup. Each voice could be given an additional data member called sequence\_num. Every time a note\_on event occurs, the voiced assigned the new note would have its sequence\_num data member updated to the current value of note\_count to indicate when it was activated. The voice with the lowest number is the oldest and should be dropped when more than the max number of notes occurs. And a uint64 is so outrageously large it would take the user playing billions of notes per second for over a hundred years before it overflows. I don't think it's fair to expect my little synth to be ready to handle integer overflow of such biblical proportions.

	
	\subsubsection{Envelopes}
	
	\begin{wrapfigure}{l}{.5\textwidth}
		\centering
		\caption{The states of the envelope and the events that trigger phase transitions}
		\includegraphics[width=.8\linewidth]{envelope_state_diagram}
	\end{wrapfigure}
	
	\paragraph{The next step in fleshing out the synth was to add some dynamic control to the voices} Musical instruments get louder and quieter over time in different ways. A percussive instrument like a drum or piano will instantly play at maximum volume. The loudness will diminish over time depending on the instrument. A bell, when struck, will ring out for a long time unless the player mutes the bell. A snare drum will go quiet almost immediately after it reaches its maximum volume. Conversely, a violin can increase in volume fast or slow and can play at the same volume indefinitely. Synthesizers have a way of emulating volume dynamics with a component called an envelope. Envelopes are a multiphase signals that can be used to modulate other signals over time. They commonly have four phases wherein the signal gain increases and decreases. These phases are known as Attack, Decay, Sustain, and Release.
	
	\textbf{Attack} is the first phase of the envelope. During the attack phase, the signal starts at 0 and ramps up to its maximum gain. Attack represents how long it takes for the gain to reach max.

	The \textbf{Decay} phase occurs immediately after the attack phase. During the decay phase, the signal gain decreases until it reaches the configured sustain value. Similair to attack, the decay phase indicates how long it takes for the gain to decrease sustain.

	The \textbf{Sustain} phase occurs immediately after the decay phase. During the sustain phase, the signal stays the same. Unlike attack and sustain, sustain does not relate to time, but simply the gain that the signal will decrease to after the attack phase is complete. As long as the note is held down on the keyboard, the envelope will remain in the sustain phase indefinitely.

	The \textbf{Release} phase occurs immediately after the key is released on the keyboard and is the final phase. The envelope can enter the release phase from any of the previously mentioned phases. Release refers to how long it takes for the envelope gain to return to 0 after the note is released. \cite{hass_2021}

	\paragraph{Envelopes can be used generically to modulate any kind of signal such as volume, filter frequency cutoff, or even the gain of an audio effect like reverb or distortion} The application of envelopes is limited to your imagination making them a powerful tool in audio synthesis. The most common application of an envelope is known as an \textbf{amp envelope}. An amp envelope refers to when the volume of a voice is modulated by the signal gain of an envelope. Multiplying a voice's output signal by the amp envelope's signal gain results in the voice growing and diminishing in volume according to the phase of the envelope. This makes the synthesizer capable of emulating the volume dynamics of many instruments. Adding an amp envelope was the next step in developement. In a polyphonic synthesizer, every voice (see previous section) must have its own amp envelope so that each note can grow and diminish independently of each other. A new class called Envelope was implemented with a state machine to represent the phases of the Envelope:
	
	\begin{minted}{c}
	//Envelope.h
	class Envelope {
		public:
		
		//State machine enum for tracking what phase the envelope is in.
		//A ready state was added to indicate that a voice was inactive and
		//therefore available to be used
		enum Phase {
			ATTACK,
			DECAY,
			SUSTAIN,
			RELEASE,
			READY 
		};
		
		float val = 0.0; //the current signal gain of the envelope
		Phase phase = Phase::READY;
		float process(); //called periodically by timer event callback to advance
		//the gain according to envelope phase state
		void note_on(); //Kicks of the envelope by settings its phase from ready to attack
		void note_off(); //Note off events will set the envelope to the release phase
		void reset(); //After release phase is completed or a new note must be played and all
		//voices are occupied, the envelope must be reset.
	};
	\end{minted}
	
	\begin{minted}{c}
	//From Synth.cpp: Shows the voice getting the next sample from its oscillators
	//and multiplying the sample by the amp envelope's signal gain to attenuate
	//the final sample before it is returned to the synth
	float Voice::get_sample()
	{   
		//the sample outputs are also divided by the number of active voices to
		//prevent blowing out speakers and deafening user
		float sample1 = (_osc1.get_sample() * _osc1_amp)/NUM_VOICES;
		float sample2 = (_osc2.get_sample() * _osc2_amp)/NUM_VOICES;
		return (sample1 + sample2) * amp_env.val;
	}
	\end{minted}
	
	\begin{wrapfigure}{r}{.5\textwidth}
		\centering
		\includegraphics[width=.8\linewidth]{amp_envelope_visual}
		\caption{Graph of the amp envelope in action}
	\end{wrapfigure}
	
	\paragraph{The Voice class was refactored to have its own instance of the Envelope class} This allows each voice to change in volume independently of when the voice becomes active. Every time the audio callback happens and the synth calls get\_sample on a voice, the voice will generate the next audio sample and return the product of the sample and envelope gain.
	
	\paragraph{The envelope does not produce any sound} So the Envelope process method does not need to be run during the audio callback. The hardware timer takes on the duty of periodically telling envelopes to advance and change their signal gain according to their current phase. The configuration of the timer is taken into account to ensure the envelopes advance correctly according to time. If attack is set to 100 milliseconds, the envelope gain should increase by .01 every millisecond during the attack phase.
	
	\subsubsection{Filters}
	\paragraph{The last major component to be implemented in the synth was signal filtering} Up until this point, the control of the harmonic qualities of synth was fairly limited. You could combine different fundamental waveforms and alter pitch to create harmonically rich signals. But the subtractive aspect of this subtractive synthesizer was conspicuously missing. Filters make up the subtractive nature of the system. They allow the user to selectively chop frequency bands from the final signal to emulate real-life instruments.\cite{welsh_2006} An synth audio filter typically has two main parameters: Frequency cutoff and resonance. Frequency cutoff (or just cutoff) determines what part of the frequency spectrum will be removed. Resonance is a gain boost that can be applied around the cutoff frequency to accentuate a certain range of frequencies in the spectrum. \cite{musictech}

	\paragraph{The most common filters used in subtractive synthesis are low pass, high pass, and band pass filters} Applying a low pass filter to a harmonically rich signal has the effect of removing the more harsh upper harmonic frequencies making the signal more mellow and deep. This type of filter is effective in producing sounds like a bass guitar, bass drum, or cello. Applying a high pass filter will cut out the low frequencies in the signal. This is useful for creating a lead instrument intended to cut through the spectrum without sounding muddy from the lower range of frequencies. Usually, a high pass filter will be used to produce sounds like a flute, violin, or snare drum. A band pass filter is a combination of low and high pass filters. Both the high and low ends of the spectrum are removed leaving a small frequency band in the middle range. I wrote a generic filter class that could be subclassed into the different types of filters. \cite{hass_2021} \cite{lacamera_2020}
	
	\begin{minted}{c}
	//Filter.h
	class Filter {
		public:
		Filter(float sampleRate, float cutoffFrequency, float q);
		virtual float process(float input) = 0;
		void set_cutoff(uint32_t freq_hz);
		void set_resonance(uint32_t gain);
		virtual void update_coefs() = 0;
		
		float cutoffFreq;
		float resonance;
		
		private:
		float sampleRate;
		float b0, b1, b2, a0, a1, a2; //These private members are the coefficients
		//used in a difference equation during processing. More on that later.
		
		/*
		These members contain inputs and filtered outputs
		from previous process cycles. x1 and x2 are the original input samples
		from the two previous process events. y1 and y2 are the filtered outputs
		from the previous two process events. They must be used in the difference
		equation
		*/
		float x1, x2, y1, y2;
	};
	
	//Filter.cpp
	//Subclasses override process and update_coefs functions according to their
	//functions
	class LowPassFilter : public Filter;
	class HighPassFilter : public Filter;
	\end{minted}
	

	\begin{wrapfigure}{r}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{filter_sequence_diagram}
		\caption{Sequence diagram illustrating how audio samples are filtered}
	\end{wrapfigure}

	\paragraph{This part of the synthesizer was the biggest unknown to me} At the time of implementation, I needed to learn more about digital signal processing and the underlying math before I could get started. I did a little research and came across the concept of filtering audio signals using the difference equation. Since my system works by generating audio one sample at a time, it made sense use to a filter design that operates on discrete samples rather than a collection of samples. After some reading and beard stroking, I understood what I needed to do. I could filter my audio stream using the \textbf{difference equation}\cite{duke_2000}. It's a relatively simple algorithm and is computationally efficient. The process of designing a filter using the difference equation is as follows:
	
	\begin{enumerate}
		\item Based on your desired filter, you must choose a method of generating appropriate filter coefficients. Some methods include Bilinear Transform, Chebyshev, Finite Impulse Response, Butterworth, etc. The desired filter frequency response will determine what option method to use.
		\item Use the chosen filter method, desired cutoff and resonance factor to generate coefficients for the difference equation. The amount of coefficients needed is determined by the filter design. More or less coefficients is not a determining factor in a filters quality or accuracy. Therefore you must generate coefficients appropriate to the filter design you choose to implement.
		\item Implement the difference equation to operate on your samples with your generated coefficients.
	\end{enumerate}

	\paragraph{After some research, I found that a method known as bilinear transform would be needed to generate the filter coefficients for my low-pass and high-pass filters} A bilinear transform function is used to map a continue continuous time transfer function to a discrete time transfer function. In this case, the discrete time-transfer function is the input samples from the oscillator. The continuous time transfer function is the filter as a second-order difference equation. The filter coefficients are generated by performing the bilinear transform based on the desired cutoff frequency and a fraction of the sample rate of the system.\cite{stanford_2007} The alpha portion of the filter represents how steep the attenuation of the signal is around the cutoff frequency of the filter. Resonance plays a part in the alpha coefficients as the resonance will factor into how much gain is applied around the cutoff frequency of the filter. Following is my code implementation of the low and high pass filters in my project with a sequence diagram to help visualize the flow of data through the filter and how previously generated values are stored for use on the next filter cycle.

	\begin{minted}{c}
	//Filter.cpp
	void LowPassFilter::update_coefs() {
		//w0 is the normalized cutoff frequency
		float w0 = 2.0f * M_PI * cutoffFreq / sampleRate;
		//Doing part of the bilinear transfer calculation here to make
		//coeffecient calculation expressions easier to read
		float cosw0 = std::cos(w0); 
		//The attenuation factor around the cutoff
		float alpha = std::sin(w0) / (2.0f * resonance); 
		//The rest of the bilinear transform applied to generate the 
		//coefficients for the second order difference equation
		b0 = (1.0f - cosw0) / 2.0f;
		b1 = 1.0f - cosw0;
		b2 = (1.0f - cosw0) / 2.0f;
		a0 = 1.0f + alpha;
		a1 = -2.0f * cosw0;
		a2 = 1.0f - alpha;
	}
	
	//This snippet of the second-order difference equation shows how
	//the output is generated based on previous inputs. The input and
	//output at the time of process are preserved and saved to x1 and y1
	//and the input/output values from the previous process cycle are
	//saved to x2 and y2. These saved inputs and outputs are used with
	//their cooresponding filter coefficients. The full effect of this
	//block shows how filtering is performed on a continous time transfer
	//domain to output samples in a discrete time transfer domain
	float LowPassFilter::process(float input) {
		float output = b0 / a0 * input + b1 / a0 * x1 + b2 / a0 * x2
		- a1 / a0 * y1 - a2 / a0 * y2;
		
		x2 = x1;
		x1 = input;
		y2 = y1;
		y1 = output;
		
		return output;
	}
	\end{minted}

	\paragraph{Adding filters to the project was a difficult concept to grasp} Because of my lack of previous experience, some concepts had to be taken with a bit of faith and proven by seeing them in action. Digital filters is such a complex concept that an entire semester of work could have be dedicated to fully understand the math behind them. This first naive attempt at filtering resulted in some stable low and high pass filters and some gained confidence. I believe now I can move forward and be able to do some deeper research into filters and implement more complex and capable filtering in the future.
	
	\subsubsection{The "little big" software features}
	\paragraph{These are features that I would say are easily overlooked when they work, but utterly infuriating when they're broken} Some features were so important that they couldn't be ignored but sucked away a critical amount of time from the actual audio processing.

	\textbf{The LCD Screen:} I set up an I2C bus and wrote a driver to communicate with the LCD allowing me to quickly print menu information to the screen. This was a tediously long process with a lot of trial and error and sucked up about 2-3 weeks of my time. But it works very well and was critical in implementing the menu system.

	\textbf{The Hardware Timer:} I set up a hardware timer that triggered an interrupt service routine for reading peripherals. This effectively is a polling mechanism because of the lack of support for GPIO  in libDaisy. This timer callback would read the analog potentiometers, the rotary encoder, and the MIDI UART line and respond to any inputs on these peripherals. This timer was also used to advance the state of the amp envelope (See envelope section for details).
	
	\textbf{The Menu:} Running with the driver code for the LCD screen, I built up a class that will display the menu and its corresponding contexts. The Menu maintains a data structure of configurable parameters for the synthesizer such as oscillator waveforms and filters. The general purpose potentiometers adaptively manipulate menu parameters under the hood according to the currently selected menu context.
	
	\textbf{Remote Control GUI Application:} A simple GUI control panel app was written in Python using the QT framework to be to remotely change parameters without the need to physically interact with the synthesizer beyond simply playing keys on the keyboard to generate sound. This was accomplished using a secondary USB cable to communicate via USB Serial to tell the synth what parameters to change. This GUI also allows the user to save custom patches to a file on the host PC's disk, which can be loaded at will to instantly reconfigure the system according to the patch. This feature is still in development but is mostly complete with the ability to change a few parameters. The USB Serial driver provided by Daisy is susceptible to failure when too much data is sent down the line too quickly. Some more time is needed to determine how to make the app play nicely with the faulty serial driver.

\section{Conclusion}
	\subsection{Successes and completed features} 
	\paragraph{I accomplished to core requirement of my project:} Implement a real-time subtractive synthesizer from scratch on an embedded device. My system uses oscillators to generate tones with pitches based on user input from a generic MIDI keyboard. The tones are combined and processed through an amp envelope to add dynamics. The output from the envelope is processed through filters to carve frequencies out of the audio signal. The final processed audio signal is played out speakers or headphones which can be heard by the user in real-time without interrupts like pops or clicks in the audio. This here is a real working synthesizer; no ifs and or buts about it.
	
	\subsection{Total Failures}
	\paragraph{I did not manage to correctly implement some features into the system} Sometimes while developing the system, I made some critical mistakes that cost me valuable time.
	\begin{enumerate}
		\item I failed to add support for saving patch parameters to flash on the device itself. Bugs were encountered with libDaisy that made this too difficult to accomplish in my timeline.
		\item It took several attempts with weeks of effort to create a handmade circuit board to contain all my hardware components. My first attempt at laying all the electrical components on a circuit board was based on making flying wire harnesses that attach to all the human input peripherals. My thinking at the time was that making flying harnesses would make it easier to put the whole system into an enclosure. What resulted was a noisy mess of wires that made me feel physically ill when I looked at it.
		\begin{figure}[H]
			\includegraphics[width=.5\linewidth]{hardware_gore}
			\caption{Behold hardware gore: My first attempt at a prototype on perfboard}
			\centering
		\end{figure}
		\item In the process of adding an external power supply capable of providing power to the MIDI input circuit and LCD screen, I accidentally wired my power source into the wrong pin on the microcontroller. I fried the device, releasing the magic blue smoke. This isn't the first time I've done this and despite my efforts to not repeat the sins of the past, it happened. For a while, I had to move forward with development and adapt to not having a microcontroller while I waited for a new one to be delivered.
	\end{enumerate} 
	
	\subsection{Lesser Failures}
	\paragraph{Several optional features were planned for the system but had to be dropped for the sake of time} 
	\begin{enumerate}
		\item General purpose Envelopes to the system that the user could route to various components to be used for modulating arbitrary signals like filter cutoff, audio effect gain, oscillator pitch, etc.
		\item Low-Frequency Oscillators (LFOs). LFOs are non-audible oscillators that are used for arbitrary signal modulation similar to the previously mentioned general-purpose envelope. You can take the generated signal of the LFO and use it to modulate things like the pitch of an oscillator to add pitch vibrato. You can apply LFOs to filter cutoff to create filter "swishing" effects. You can apply it to the gain of the left and right audio outputs to create a panning effect between the left and right channels.\cite{paris_2022}
		\item Support for additional MIDI input messages were planned like the pitch and mod wheel devices often built into keyboards.
		\item An audio FX chain on the end of the pipeline to add effects like reverb, echo and distortion. These audio FX can add spatial features and additional audio coloring, expressiveness and complexity that users tend to enjoy playing with most.
	\end{enumerate}
	
	\subsection{Hindsight: "Choices" were made} 
	\subsubsection{The Daisy Platform:} Choosing to base my master's project on the promising platform of Daisy had serious consequences on development. Daisy was a very attractive option that promised a system that relive me of the more frustrating hardware complexities and make it easier to write audio processing software. The truth is, this is a new product. It focuses on implementing features to make development easier for hobbyists so they can make smaller audio FX processors and instruments. I would have been better off choosing a more mature audio-centric board from ST Microelectronics and using the tried and true STM32Cube IDE and setup the hardware myself.
	\subsubsection{Hardware Implementation:} Too much time was spent trying to migrate my project off of a breadboard and onto a permanent circuit board. I thought I knew enough of what I was doing that I could move my project off the breadboard and into a permanent enclosure without any intermediate steps or revisions. The flying harness prototype was a time vortex of disaster. After all that effort, what I had made was garbage and it now sits in a drawer. It's new function is to remind me to prototype incrementally and plan ahead.
	\subsubsection{Spaghetti Code and Tight Hardware Coupling:} As the system rapidly developed, some holes in previously developed features became more obvious as the system grew. These issues had to be ignored in favor of moving forward with development to meet the deadline. At times, refactoring software would create regressions that would cost even more time to fix. Some software refactoring and reorganization would help to make it faster and easier to add additional functionality to the system. In hindsight, I wish that I had designed the audio components to be more modular. If they had no hardware dependencies, I could easily test them out without the microcontroller and the synth system as a whole. Software fixtures could have been written to run unit tests on individual audio components. I believe this would have resulted in more rapid audio development first deploying and testing on my host machine without hardware in the loop. Once I was satisfied, the code could be integrated with the device firmware.
	\subsubsection{Performance Issues and Bottlenecks}
	Overall, there doesn't seem to be much of a bottleneck in the audio pipeline within my project. The synth is never late to deliver audio samples to the output DAC, so there are never any audio crackles or pops in the final audio played out the speakers. Many of the CPU clock cycles are spent just waiting for the audio and timer callbacks. The only major performance issue that currently persists is the noisiness of the potentiometers. Sometimes I feel like the potentiometers will get super noisy just by looking at them wrong. A software filter was written to smooth out the readings prevent responses to false user input and has reduced the frequency of these false events. It still does not completely stop the events altogether. Adding a hardware low pass filter to the potentiometers couples with the software filter may be the silver bullet to solve this problem for good.
	
\section{Conclusion}
	\paragraph{This project was a success, but not nearly the success that I envisioned} I spent countless hours searching my code for software bugs and probing hardware pins with a multimeter and oscilloscope. Many late nights were spent in the company of a soldering iron, protyping hardware, and writing tests to check that it was working. Several weeks were spent writing and testing drivers to interfaces with the LCD screen. A week was lost trying to add flash memory storage. On the other hand, many hours were spent with the exhilarating feeling of adding a new audio processing feature to my system. Sometimes these wins would come one after another. Soon thereafter, a regression would happen requiring hours of hard work to debug.
	
	\paragraph{Nevertheless, I completed my task}. I have a system that works. I can plug in a midi keyboard and play notes and hear them play out my headphones as I would expect. I can dive into the menu and adjust the envelope, filters, and waveforms and it works the same as any commercial off-the-shelf synth that I have used. Most importantly, I have learned critical lessons about the process of implementing a complex embedded system from the ground up: The importance of carefully selecting your hardware platform, incremental hardware prototyping, maintaining modular firmware code, efficiency practices and design philosophies. I've gained confidence to venture more boldly into other foreign domains of software development such as video processing and hardware driver development. In implementing this system, perhaps an entire four-year undergraduate degree was learned about the software architecture and the pitfalls that must be avoided: failure to develop code modularity, obsessing over adding new features and completing multiple tasks in one step. I learned about the wisdom of focusing on making the system work while still paving a path to extend it.
	
	\paragraph{The retrospect of this paper is almost cathartic} It provides so much clarity on the events of this project that I've devoted the larger part of a year to. Surprisingly, my next steps I'm inclined to do not involve finishing the project where it stands now. I want to preserve the code from this project and use it as a reference and redo this whole project from the ground up. I want to take my audio software components to be independant of the hardware platform. I want to write create software fixtures to test these components outside an embedded environment. I would then reintegrate these components into a new implementation of my synthesizer but on a more mature and robust embedded platform like an STM32. I would then implement all the missing features from this project like the LFO, audio fx components, patch saving, etc to complete my final vision. The only remaining step would be to design and fabricate my own printed circuit board professionally and put the whole thing in a fancy box and show it to the world. I'm excited at this prospect and look forward to continuing my efforts to deepen my knowledge audio processing and the world of embedded systems.

\section{Acknowledgements}
	\paragraph{I would like express my gratitude:}
	\begin{enumerate}
		\item To my wife and children who have been so supportive and patient throughout the whole process. Their love and affection gave me the strength to push through the numerous late nights where I stayed up past 3 writing software and slaving over my soldering work bench.
		\item To my friends and family who showed interest and support in my project and at times offered me advice and encouragement.
		\item To Dr. Frank Jones, my mentor for this project. Until he arrived at UVU, I was constantly begging the school faculty to bring back the Advanced Embedded Systems course to the master's curriculum. Frank's arrival and efforts to revive the embedded systems course is the reason I was able to move beyond blinking LED's on an Arduino and learn the ropes of true embedded systems design.
		\item To Utah Valley University. I've studied at this institution since 2014 and will have graduated twice from it come next week. I got my undergraduate degree in Commercial Music and discovered engineering in the process. I continued to take classes until I fulfilled requirements to start the Master's of Computer Science program. Now I am at the end of my graduate program and enjoyed the benefits of a lucrative and fulfilling career that is continue to grow. I owe so much to UVU for their openness to all who wish to learn and their dedication to educating and uplifting its students.
	\end{enumerate}

	\paragraph{I would also like to give credit to all the silent hero's of the computer science and open source community} W stand on the shoulders of giants. And the things we accomplish now are only possible because of the charitable efforts of other engineers. The free software tools, compilers, blog posts, instructional videos and documentation are indispensable assets that have made the joy's of computing accessible to nearly every human being in the world. The development of low cost hardware, high level programming languages and free learning resources made it possible for me to discover my passion for computers. It gave me the courage to leave my career path as a profession musician that was leaving me jaded to music and creatively unfulfilled. Now, my preferred choice of artistic expression is through the medium of engineering. I owe so much to the millions of people who contribute to the open source world making computer science accessible to everyone.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sources.bib}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{User's Manual}
\subsection{Hardware}
\paragraph{Do I really need to explain how to construct the hardware? If so, is a guide for which pins to solder good enough?}
\subsection{Compiling and flashing the firmware}
\paragraph{In order to compile and flash the firmware you will to need the following}
\begin{enumerate}
	\item A host machine
	\item The GCC toolchain for the ARM architecture
	\item The Make tool to run the compilation
	\item The Device Firmware Update tool (dfu-util) to flash the firmware
	\item The project source code
	\item A Daisy Seed development board from Electro-Smith
	\item A micro USB cable
\end{enumerate}

\paragraph{Compilation:}
\begin{enumerate}
	\item On your host machine, clone the source code for this project from github by running: \textit{git clone https://github.com/thealienthing/shiny-octo-train}
	\item '\textit{cd}' into the repo and run the command: \textit{git submodule update --init --recursive}. This will ensure that the libDaisy HAL library is cloned as it will need to be compiled first.
	\item '\textit{cd}' into libDaisy and the command: \textit{make}. libDaisy will be compiled into a statically linked library that will be used in the compilation step for the synth firmware.
	\item After libDaisy has been compiled, '\textit{cd}' out of libDaisy and run the command \textit{make} to compile the synth firmware.
\end{enumerate}
\paragraph{Flashing the firmware:}
\begin{enumerate}
	\item After the firmware binary has been compiled, the development board must be connected to the host machine and put into bootloader mode. Connect the micro USB cable between the host machine and the Daisy development board. There are two buttons on the Daisy board: One labeled boot and the other labeled reset. Hold down the boot button and press the reset button while still holding down the boot button. Order is important. The power LED should go dim while the board is in bootloader mode.
	\item Now that we've put the Daisy into bootloader mode we can flash the firmware. In the top directory of the repo, run the command \textit{make program-dfu}. A text progress bar should print to the terminal to display the progress of the flash sequence. Once the flash sequence is completed, the program will exit and the board is ready to play music.
\end{enumerate}

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.

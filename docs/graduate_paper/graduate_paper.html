<!DOCTYPE html>
<html>
<head>
<title>graduate_paper.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="uvu-mcs-graduate-paper">UVU MCS Graduate Paper</h1>
<h3 id="candidate-benjamin-stoneking">Candidate: Benjamin Stoneking</h3>
<h3 id="advisor-dr-frank-jones">Advisor: Dr. Frank Jones</h3>
<h1 id="abstract">Abstract</h1>
<p>The project focuses on implementing a real time digital audio synthesizer from the ground up on a micro controller. The aim is to leverage all the strengths of the microcontroller such as interupt service routines, timers and hardware interupts to produce a system capable of acting as a reliable musical instrument for the user. The topic is interesting as an exercise in implementing an audio data processing pipeline that involves many signal processing components such as oscillators, pitch control, gain mixing and attenuation, envelope processing and filtering to produce a variety of different sounds. The project represents building a complex system capable of being embedded in a limited processing space with options to extend its capabilities. The final project yielded a synthesizer meeting the fundamental requirements to be called a subtractive synthesizer: It can produce sound based off input from a midi instrument, can manipulate the signal gain to create an instrument capable volume dynamics. It can process the audio through high and lowpass filters to change the timbre to sound similar to real world instruments like guitars, pianos and string instruments and it has a hardware interface consisting of knobs, digital encoders and screen allowing the user to directly manipulate the sounds in real time. A computer software accessory application can communicate with the system to be able to adjust parameters, save and load user configurations for later use. Although the project meets minimum requirements for being a synthesizer, it has much work to be done. Many lessons were learned: the process of prototyping and implementing hardware peripherals can be fraught with frustrating bugs and failures. Embedded low level software has a much more involved and timeconsuming process for debugging, with regression tests being an absolute necessity. A lot of the funcionality of electronic musical instruments that we expect is a harder system to implement that expected. The lines of code for this system ended up around 2250 which is much lower than what is typically required of the project. Much of the reason for the smaller count of lines of code was because 1) Much time getting the system working required many if not tens of hours writing drivers to interface with hardware without the system regressing 2) More lines of code is not a good idea in an embedded system as it causes the firmware builds to grow and the microprocessors flash memory cannot contain all the instructions and 3) 50-100 hours of time was taken away from software development for prototyping hardware on breadboard and then designing and implementing permanent hardware fixtures with many setbacks and revisions required.</p>
<p>Keywords: audio, systhesis, embedded, microcontroller, real-time, midi, hardware, filters, envelopes</p>
<p>Begin with a proper thesis statement, which may be several paragraphs. A proper thesis statement answers the
following 4 questions:
(1) What problem are you trying to solve?
(2) why is it interesting?
(3) What are your major results?
(4) What are your conclusions?</p>
<h1 id="introduction">Introduction</h1>
<p>The problem I wanted to solve was rather simple: The acquisition of musical gear is common obsession that plagues millions of musicians around the world. The cost of gear for performance or music production quickly drains the wallets of musicians who already struggle to keep their bank accounts at a non-zero value and for many musicians, this thought crosses their mind at least once: &quot;Can I just make my own X and save some money?&quot;. If you ask anyone who has gone down the road of making their own digital software and music hardware, they will tell you the answer is &quot;Yes you can just make your own X. No you won't save any money. You will spend more&quot;. Over the years, I've researched this very topic and found this conclusion to be true but with a small caveat: You <em>can</em> in fact save money by making your audio effect of electronic instrument as a digital system rather than an electrical circuit of discreet analog components. This caveat has it's own caveat: if time is money, then every hour of time spent in implementing your system is implicitly adding to the price tag of your system.</p>
<p>I've known these facts for a long time and have seen it in action as my education has had me walk through the process of making yet another database management system, yet another virtual machine. Unsuprisingly, my database hasn't replaced MySQL and my virtual machine hasn't replaced the JVM. Still, so much knowledge and understanding of how computers work was gained from these experiences. I applied this same principle to my experience as a musician with the goal of my masters project: Make yet another digital synthesizer on an embedded microcontroller. The purpose is to learn how implement a complex realtime embedded system, develop a digital audio pipeline, make a hardware interface to control it and indulge my fantasy of making my own piece of musical gear. In order to maximize the learning potential of this project and focus in on the process rather than the final destination, I decided to set a few restrictions:</p>
<ol>
<li>The firmware must be implemented in a low level language such as C/C++.</li>
<li>I can only use the standard C/C++ libraries and a Hardware Abstraction Layer library to minimize time wasted fussing with getting hardware peripherals communicating with the microcontroller.</li>
<li>No libraries means no DSP libraries. Every bit of audio data music be generated by instructions hand crafted by me. Although I can reference other digital audio codebases, I avoided it as much as possible to allow myself a chance solve the problem on my own for better or for worse.</li>
</ol>
<p>The resultant product of my project turned out to be satisfactory but fell miles short of where I hoped it would be. I believe my project currently is in a place where it can be considered something that meets the basic requirements to be called a subtractive synthesizer. It's relatively stable, but with a few tweaks stands to be a studio or stange performance ready instrument for musicians.</p>
<h1 id="concepts">Concepts</h1>
<h2 id="a-quick-word-on-audio-synthesis-and-features-required">A quick word on audio synthesis and features required</h2>
<p>Audio synthesis is a rather broad topic and a project goal of &quot;make a synthesizer&quot; is very vague. There are so many styles and methods of audio synthesis. Since this is my first jump into the world of real time data processing, I decided to make my synthesizer as classic as possible. The system I designed is a <strong>subtractive</strong> synthesizer: fundamental waveforms are combined and then passed through a series of filters to strip away certain frequencies to create new sounds. This method is a relatively primitive yet classic means mimicking real world instruments and dates back nearly a century. Subtractive synthesis was made especially popular in the 60's and 70's for its ability to bring new and imaginative sounds to rock and jazz music.</p>
<p>In order to make a digital subtractive synthesizer that meets the bare minimum requirements, I needed to implement the following features for my microcontroller:</p>
<ol>
<li>Oscillators - produces an audio signal by repeating some pattern such as a sine wave at a set frequency</li>
<li>MIDI - <strong>Music Instrument Data Interface</strong> or MIDI is the standard way that musicians communicate with electronic instruments and software. A common electronic piano keyboard usually has a MIDI out jack on it allowing the user to control other devices using the keys, knobs and buttons on their keyboard. My system needs to be able to receive generic MIDI messages commanding it to turn notes on and off in real time</li>
<li>In addition to MIDI capability, the system must have 8 voices, meaning it can play up to 8 notes at once. 8 is a rather arbitrary number but is generally an acceptable amount for most keyboard players.</li>
<li>Filters - After oscillators, filters are the most critical component in a subtractive synthesizer as it allows the user to shape and mold their basic fundamental frequencies into new sounds.</li>
<li>Envelopes - After Oscillators and Filters, envelopes represent a critical component to a synthesizer. Envelopes are gates that may be triggered by arbitrary signal sources such as a key press on the keyboard and over time, the envelope gate will open and close over time. Most commonly, an envelope is applied to the output gain of a synthesizer allowing control of the volume which adds a great deal of strength to the synthesizers ability to mimick other instruments.</li>
</ol>
<h1 id="architecture">Architecture</h1>
<p>A big decision in implementing this project was choosing the right microcontroller platform. I considered several options but settled on a popular and relatively new audio development eco-system by Electro-Smith known as Daisy. Daisy is an Arm Cortex M7 based development board that provides all the necessary processing power and peripherals necessary for audio processing and then some. The Cortex M7 clocks at 480Mz peripheral support for an extremely high resolution digital to audio converter (DAC). In addition to the DAC, it has support for expected devices such as I2C, UART, USB, Timers etc. The biggest plus (at the time) to choosing Daisy as my platform was Electro-Smiths libDaisy library which acts as an abstraction on top of the standard HAL libraries for the Cortex M7. This was an attractive option as it would allow me to set up hardware quickly and allow me to focus primarily on the audio and signal processing aspects of the project.</p>
<img src="images/daisy_promo_pic.png" alt="Daisy" height=150 title=Daisy>
<p>They also provide a DSP library which I chose to not use as an established requirement was not to use a DSP library and instead do all my own audio processing from scratch. As I mentioned before, Daisy seemed to be a great choice at the time and definitely freed up time in initial hardware setup. What I was not aware of was the relative newness of the platform. Daisy originally was a Kickstarter funded project which receive incredible amounts of success and support from customers. The platform was designed with musicians in mind, and they put a lot of work into supporting much more user friendly development languages and frameworks like MaxMSP, Puredata and Arduino to assist new developers in making their project and forgetting about all the gory details. Because of this additional layer of abstraction, it meant that a lot of time was put into customer accessibility and full support for utilizing all hardware to its fullest capabilities was put off and is still in current development. This had the following consequences for my project:</p>
<p>Some HAL library function did not work as they were supposed to.
a. One of the biggest disappointments was a lack of support for two way USB communication outside of the bootloader. When I was implementing my serial interfacing GUI application for remote control of my project, I discovered that the USB software support extended to either a dedicated interface for the device to either receive or transmit data over USB but not both. My choices were to either hack a software solution involving handing off control between the host computer and device, adding the two USB communication support to the libDaisy library (something that I did not have time or sufficient knowledge of USB to tackle) or simply add a USB-UART converter chip and connect it to some UART pins on the devices. I opted for the last option as I was severly lacking in time.
b. Another severly lacking feature was support for hardware interupts. Some of the HAL library provided support for hardware interupts for USB receive (this felt insulting as I could not talk back on the USB line). But also, there was no support for UART and GPIO interupt service routines (ISR). This is the reason why MIDI data events are handled via periodic polling during timer ISRs. I believe this polling method yielded imperfect MIDI data consumption.
c. A last gripe I have with the platform is the datasheet claiming availability of certain I2C and UART lines on the boards pins, but they are in fact not completely wired up in the boards current revision. This was not documented and instead made known to me via community forums. This lack of complete availability of hardware made adding all my peripherals rather difficult because of pin conflicts.
This whole experience soured me a bit to the Daisy platform in general. While I admire the ambition of the project to support programming in a variety of languages that opens up audio development to new developers, I feel it would have been best to fully flesh out all hardware support in basic C before moving on to supporting things like Object Oriented Programming, visual programming languages and Arduino.</p>
<p>My original vision for the project was, as the masters capstone prompt recommend, <strong>very ambitious</strong>. My vision was to implement a near clone of my first synthesizer, the Korg microKorg, but as a headless unit that could be used by plugging in any standard MIDI keyboard (In my original design drawing, you can even see an array of buttons that represented a very limited keyboard which would allow the user to play notes without connecting an external keyboard. I truly was a bright eyed summer-child when I first envisioned this project).</p>
<p>I'll start the explanation fo my design first with the things that were complete, and then address the absent hardware and software features.</p>
<h2 id="hardware">Hardware</h2>
<p>This section covers the hardware was successfully integrated into the project</p>
<h3 id="midi">MIDI</h3>
<p>There is a 5 pin MIDI input circuit that allows the user to connect a standard MIDI cable between the MIDI out of a standard keyboard and my synthesizer. This functionality is plug and play. By plugging in the MIDI cable and powering on the device, you can immediately start playing notes and making music which you can hear by pluggin in headphones or a speaker into the 1/8&quot; audio jack.</p>
<div style="text-align:center">
    <p>Midi Optocoupler Circuit</p>
    <img src="images/midi_optocoupler_circuit.png" alt="Daisy" height=300 title=Optocoupler>
</div>
<p>The MIDI circuit implements a standard of device safety for receiving incoming signals. An optocoupler receives signals and converts the incoming electrical signal from the MIDI line into an optical signal which can be read and converted to an electrical signal that is safe for the device. This safety precaution is meant to handle cases where the connected keyboard is malfunctioning or outputing an electical signal that is to high voltage for the pins on the MCU. It also acts as a much easier to replace component in the circuit. If there the keyboard is outputing voltage dangerously outside the expected voltage range, the optocoupler will fry and can be easily replaced instead of frying the MCU.</p>
<h3 id="analog-potentiometers-and-screen">Analog Potentiometers and Screen</h3>
<p>For adjusting parameters, I took a design philosophy of the Korg microKorg and setup an array of 5 analog potentiometers that had contextual control over various synth parameters which were determined by the internal state of the synthesizer. The internal state of the synthesizer is communicated to the user with a 20x4 character LCD display that shows the user the current mutable context of the system.</p>
<div style="text-align:center">
    <p>Photo Of Screen Main Menu</p>
    <img src="images/placeholder_duck.png" alt="Daisy" height=100 title=Duck>
</div>
<p>For example, if the menu is displaying the oscillator control context, you can use the potentiometers to change the fundamental wave forms generated by the oscillators and set them as you like to a Sin, Triangle, Square, Sawtooth or Noise generator. If the menu was instead displaying the Amp Envelope control context, you can use those same knobs to adjust the Attack, Decay, Sustain or Release parameters of the envelope. The way the menu context is controlled is via a digital rotary encoder with a built in push button. Turning the knob will take the user out of their current control context into a universal menu of contexts that the user may control. They can turn the knob to highlight the component they wish to control (Filter, Envelope etc) and engage the encoder push button to transition to the control context of the highlighted component. The potentiometers would be mapped to adjust the fields within that context and perform specific interpolations of the components. Since some parameters are better controlled logarithmicly, the integer value read from the knob can be converted to a float and scaled according to its required range. Some parameters only represent a few discrete values. The large values of the 16 bit numbers would instead be mapped to the smaller ranges required to represent the discrete values of the menu parameter.</p>
<div style="text-align:center">
    <p>Photos of changing params</p>
    <img src="images/placeholder_duck.png" alt="Daisy" height=100 title=Duck>
</div>
<h3 id="power">Power</h3>
<p>The 20x4 LCD screen and the optocoupler on the MIDI input circuit require 5v, which Daisy does not put out (another frustrating aspect of the device since there is 5v power available on the USB jack of the daisy); so I added a standard breadboard power supply to the circuit accepts a barel jack input with voltages between 5v and 12v. This voltage is fed into the VIN pin on Daisy and is used to power the entire circuit and all its peripherals. A 1/8&quot; inch stereo audio jack in connected to the left and right DAC pins on the MCU and outputs a signal at line level gain out of the box without the need of an amplifier (a very nice feature). Finally, there is a USB to Uart converter breakout board connected to a set of RX/TX UART pins on the MCU which allows two way communication between a computer and the synthesizer via a remote control GUI a wrote in Python.</p>
<h3 id="audio-dac">Audio DAC</h3>
<p>Daisy has a built in high resolution digital to analog converter capable of outputting audio samples at rates up to 96khz and 24 bit depth which goes well beyond the industry standard quality of 44.1khz and 16 bit depth. There are four DAC channels: two for stereo input and two for stereo output. In this project, I only use the stereo output as processing audio input is beyond the scope of this project.</p>
<h3 id="uart-i2c-and-16-bit-input-adc">UART, I2C and 16-bit input ADC</h3>
<p>General purpose pins were configured to setup UART, I2C and ADC communications. Two separate UART lines were used for setting up MIDI input and plain serial communication for controlling the system remotely. The ADCs were used for reading the potentiometers. Some GPIO pins were used to read the digital rotary encoder for menu navigation.</p>
<div style="text-align:center">
    <p>Diagram showing wiring of peripherals</p>
    <img src="images/placeholder_duck.png" alt="Daisy" height=100 title=Duck>
</div>
<h2 id="software">Software</h2>
<p>This section goes into depth of the various software components that make up the actual audio synthesis and processing. I will address the software components in the order that they were implemented over the course of the project, as it will also help to highlight the process of generating audio from the initial press of the eventual output of the audio to a speaker. I will cover the topics of Oscillators, Voices, Envelopes and Filters completely as these comprise the critical components of the actual synthesis aspects of the project. I will only very briefly discuss other features in a few sentences as they have less to do with audio synthesis and more to do with overall system cohesion and functionality.</p>
<h3 id="oscillators">Oscillators</h3>
<p>The most fundamental part of any subtractive synthesizer is the oscillator. Generally speaking, an oscillator is simply a function generator. The oscillator will maintain a variable representing the phase of the function cycle. The sequence chart below illustrates how audio samples are generated from the oscillator and then placed into the output buffer of the DAC when it triggers an ISR.</p>
<img src="images/oscillator_sequence_diagram.png" alt="Daisy" width=400 title=Daisy>
<p>Note: The diagram above will generate a 1hz sin wave which isn't actually audible and we'd like to hear our music if possible. So the oscillator must maintain a variable containing the desired frequency and calculate our phase delta so we can actually generate the function at the desired frequency. <strong>Phase delta</strong> is a value by which we increment the phase in order to get the next sample of our waveform. To generate a wave at 1hz with a sample rate of 44.1kz, we simply need to divide two pi by 44100. Starting with our phase at zero, if we add our phase_delta to phase 44,100 times, we will have a value of two pi. Two change the pitch, we just need to multiply the phase delta by our desired frequency. If our pitch is set to 100hz, our calculated phase delta will allow our oscillator function to cycle 100 times in a second.</p>
<h3 id="what-about-other-waveforms-sin-waves-are-boring">What about other waveforms? Sin waves are boring.</h3>
<p>This is true. Sin waves generally lack harmonic complexity and will generally produce very mellow tones. The oscillator class also maintains an enum describing what <em>kind</em> of oscillator it is. The oscillator wave form can bet set with the set_waveform method. Next time get_sample is called, the cooresponding waveform function will be called returning a sample from your fancy new harmonically rich waveform. For more information about this functions, the snippet below is a good start. Also see Oscillator.cpp for full source.</p>
<pre class="hljs"><code><div><span class="hljs-comment">//Oscillator.h</span>
<span class="hljs-keyword">enum</span> WaveForm {
    Sin,
    Tri,
    Saw,
    Square,
    WhiteNoise,
    WaveFormEnd
};

<span class="hljs-comment">//Oscillator.cpp</span>
<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">Oscillator::set_waveform</span><span class="hljs-params">(WaveForm waveform)</span>
</span>{
    _waveform = waveform;
}

<span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">Oscillator::get_sample</span><span class="hljs-params">()</span>
</span>{
    <span class="hljs-comment">//Some context of this function removed for brevity</span>
    <span class="hljs-keyword">switch</span> (_waveform)
    {
        <span class="hljs-keyword">case</span> WaveForm::Sin: {
            sample = sinf(_phase);
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-keyword">case</span> WaveForm::Tri: {
            t   = <span class="hljs-number">-1.0f</span> + (<span class="hljs-number">2.0f</span> * _phase * TWO_PI_RECIP);
            sample = <span class="hljs-number">2.0f</span> * (fabsf(t) - <span class="hljs-number">0.5f</span>);
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-keyword">case</span> WaveForm::Saw: {
            sample = ((_phase * TWO_PI_RECIP * <span class="hljs-number">2.0f</span>) * <span class="hljs-number">-1.0f</span>) * <span class="hljs-number">-1.0f</span>;
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-keyword">case</span> WaveForm::Square: {
            sample = (_phase &lt; _half_cycle) ? <span class="hljs-number">1.0f</span>: <span class="hljs-number">-1.0f</span>;
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-keyword">case</span> WaveForm::WhiteNoise: {
            sample = noise.<span class="hljs-built_in">process</span>();
            <span class="hljs-keyword">break</span>;
        }
        <span class="hljs-keyword">default</span>: {
            sample = <span class="hljs-number">0.0</span>;
            <span class="hljs-keyword">break</span>;
        }
    }
    <span class="hljs-comment">//More context removed here for brevity</span>
    <span class="hljs-keyword">return</span> sample;
}
</div></code></pre>
<p>When the basic oscillator was set up and I was able to statically compile and upload code to make my little daisy output a continue 440hz sin wave, I felt a rush of excitement. It was incredibly gratifying to <strong>hear</strong> my code working instead of just seeing it in a terminal or GUI. I played around, writing little loops and time delays to change the pitch and hear the pitch change in real time and play scales. After that, I figured out the functions to generate the square function: a simple on/off function the sets the sample to 1.0 during the first half of the phase and the flipping it to -1.0 during the second half. Writing sawtooth and white noise functions was easy. I never was satisfied with the triangle waveform. Its supposed to stradle the line between a smooth sin wave and a choppy sawtooth in terms of complexity but it always sounded more like a slightly droopy sawtooth.</p>
<p>After the oscillators were properly implemented, I added the MIDI message processing through the MIDI UART connection. As MIDI messages came through the data line, the pitch data was parsed and converted into the note frequency in hz and the pitch was set on the oscillator. If a MIDI event message came through indicating that the note was released on the keyboard, the pitch of the oscillator was set to 0hz to silence the oscillator. At this point, I had the most bare minimum definition of a synthesizer. The user could play notes on the keyboard, and the configured oscillator waveform playing the cooresponding frequency would play out the speakers. But at this point, I could only play one note at a time and could not generate any signals more complex than the 5 basic oscillator functions. Adding polyphony and oscillator mixing was the next feature to add.</p>
<h2 id="voices">Voices</h2>
<p>In the world of synthesizers, single discrete note playing a certain pitch is called a voice. When your synth is monophonic, it means it only has one voice and can therefore only play one pitch at a time. Most modern synthesizers, whether analog or digital, have polyphonic capabilities. Many analog synthesizers can play four to eight notes at once, because adding more voices requires more discrete hardware oscillators and can make the system more complex, expensive and heavy. Digital synthesizers are able to accomodate more notes quite easily and will often support at least eight voices with options to play as many as 128 (the maximum number of discrete notes available in the MIDI protocol) which is quite overkill. I did a little bit of playing on a synthesizer of my own and payed attention to how many notes I needed at my most advanced and complex playing. I decided that eight would suffice with the option to add more if needed.</p>
<p>I wrote a class intended to capture the functionality of a voice. The class would contain two separate oscillators that can be set to any fundamental waveform. It would also contain separate gain parameters for attenuating or amplifying each individual oscillator. At the end of the processing chain for the voice, the attenuated oscillators would combined into one sample and passed to the classes output. Also included in the class is the option to configure a pitch offset to the second oscillator; another common synth feature in synthesizers. By detuning one oscillator from another, you can generate even more complex waveforms with frequency beating, added intensity or even complete disonance if desired.</p>
<img src="images/voice_class_diagram.png" alt="Daisy" width=400 title=Daisy>
<p>Once this class was implemented, I wrote a larger class called synthesizer that would own a group of voice instances. As keys on the keyboard were pressed and released, the synthesizer would take a voice that didn't have a pitch set to 0hz and set the pitch to that note. Some optimization was made to ensure that if a voice wasn't in use, the synth class wouldn't bother telling the voice to generate a sample and waste time on calculating the outputs of the oscillator functions. Implementing this opens a hole can of worms. What data structure should be used to hold these voices? There are many considerations to be made but first I just focused on making it work. For this project, I decided to avoid any C++ features other than simple classes and subclassing, as there would be performance and memory usage overhead prices to pay that would add up over time. I chose to use a primitive array. My approach was relatively simple. As a new note was played, I would iterate through the voices until I found a voice without a pitch set to 0, and set its pitch to the new note. When note off events come in, iterate over the array until the voice with a pitch matching the note off event is found and set it to 0 to make it inactive. That was pretty easy. This was even quick to process. Most of the time you wouldn't have to iterate close to the end of the array.</p>
<p>The real rub was to handle when all voices were active and more note on events occurred. Should the synth just ignore that note, or should it drop some other note and replace it with the newest one? Most synths will change the pitch of the oldest voice in the sequence to be dropped from the note combination and use that voice to play the new incoming note. I decided to stay true to convention and implement this policy. So how do I indicate the oldest note in the sequence? My first approach to this was honestly a poor choice. It involved a rather clunky policy of making sure the voices in the array were sorted according to the order in which the pitches were played on the keyboard. Every time a note off event occured, I would have to shuffle around the voices to make them preserve the order in which they occured shuffle the deactivate voice to the back of the array. In hindsight, this approach was so bad, it's mind boggling. I never experienced any break down in the audio output because of time spent sorting, but eventually cracks started to appear when many notes were being played and released in quick succession. Sometimes, the sorting could take long enough that midi events would miss being consumed or some bug in my logic resulted in not off events not being registered; and that voice would play forever until I found the note that was stuck and then played the key again so that the note off event would clear the voice.</p>
<p>In the end, I reverted my voice organization to simply iterating through the array to turn voices on and off and just ignore events when too many notes were being played. The contrived sorting strategy was dropped and the system instantly became more stable. As I've reflected on this experience, I suspect that the best approach to handle dropping moments when too many notes are being played is to implement a doubly linked list to make it easier to maintain a sorted list that can move arbitrary voices to the back of list when they go inactivate without needing to sort.</p>
<p>At this point in the system, my software architecture looked like this:</p>
<img src="images/voice_graph_diagram.png" alt="Daisy" width=400 title=Daisy>
<h2 id="envelopes">Envelopes</h2>
<p>The next step in fleshing out the synth was to add some dynamic control of the volume voices. Different instruments get louder and quieter over time in different ways. A percussive instrument like a drum or piano will instantly play at maximum volume and the loudness will diminish over time. A bell when struck will right out for a long time until the user mutes the bell. A snare drum will go quiet almost immediately after its reached its maximum volume. Conversely, a violin will ramp up its volume a bit slower and can play at the same volume indefinitely. Synthesizers are able to have this kind of dynamic control through a component called an envelope. Envelopes are a signal that can be used to modulate a signal over time and often have four phases in which the signal gain increases and decreases. These phases are known as Attack, Decay, Sustain and Release.</p>
<h3 id="attack">Attack</h3>
<p>The phase where the signal starts at 0 and increases to its maximum gain. The attack phase indicates how <strong>how long</strong> it takes for the gain to reach max.</p>
<h3 id="decay">Decay</h3>
<p>The phase occuring immediately after attack where the signal gain decreases overtime until it reaches <strong>sustain</strong> gain. Like attack, the decay phase indicates <strong>how long</strong> it takes for the gain to decrease sustain.</p>
<h3 id="sustain">Sustain</h3>
<p>The phase occuring immediately after decay where the signal will remain at its configure sustain gain. Unlike attack and sustain, sustain does not relate to time, but simply the gain that the signal will remain at after the gain has hit its max. As long as the note is held down on the keyboard, the envelope will remain in the sustain phase indefinitely.</p>
<h3 id="release">Release</h3>
<p>The final phase of the envelope, the release phase occurs immediately after the key is released on the keyboard. The envelope can enter the release phase from all of the previously mentioned phases. Release refers to <strong>how long</strong> it takes for the envelope gain to return to 0 after the key released.</p>
<img src="images/amp_envelope_visual.png" alt="Amp Env" height=400 title="Amp Env Visualized">
<p>Envelopes can be used generically to modulate any kind of signal such as volume, filter cutoff frequency or gain on an audio effect like reverb or distortion. The application of envelopes is limited to your imagination. The application of an envelope to volume is known as an Amp Envelope and is the most common. In order to make the synthesizer capable of emulating the volume dynamics of many instruments, the next step was to add an Amp Envelope. In a polyphonic synthesizer, every voice (see previous section) must have its own Amp Envelope so that each note can grow and diminish in volume as real life instruments do.</p>
<p>A new class called Envelope was implemented with a state machine emulating the phases of the Envelope:</p>
<pre class="hljs"><code><div><span class="hljs-comment">//Envelope.h</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Envelope</span> {</span>
<span class="hljs-keyword">public</span>:
    <span class="hljs-comment">//Enum for tracking what phase the envelope is in</span>
    <span class="hljs-keyword">enum</span> Phase {
        ATTACK,
        DECAY,
        SUSTAIN,
        RELEASE,
        READY <span class="hljs-comment">//This became the new way of knowing if a voice was available for use</span>
    };

    <span class="hljs-keyword">float</span> val = <span class="hljs-number">0.0</span>; <span class="hljs-comment">//the current signal gain of the envelope</span>
    Phase phase = Phase::READY;
    <span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">process</span><span class="hljs-params">()</span></span>; <span class="hljs-comment">//called periodically by timer event callback to advance the gain according to envelope phase state</span>
    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">note_on</span><span class="hljs-params">()</span></span>;
    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">note_off</span><span class="hljs-params">()</span></span>;
    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">reset</span><span class="hljs-params">()</span></span>;
    <span class="hljs-comment">//Some setters and getters for accessing the private variables not included in this snippet</span>
    
<span class="hljs-keyword">private</span>:
    <span class="hljs-keyword">uint16_t</span> _attack_ms, _decay_ms, _release_ms;
    <span class="hljs-keyword">float</span> _sustain;
};
</div></code></pre>
<p>The voice class was refactored to have its own instance of the Envelope class so each voice grow in volume independently depending on when the voice becomes active. Every time the audio callback happens and the synth calls get_sample on a voice, the voice will return the product of its generated audio sample and the gain of the envelope.</p>
<pre class="hljs"><code><div><span class="hljs-comment">//Envelope.cpp</span>
<span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">Voice::get_sample</span><span class="hljs-params">()</span>
</span>{   
    <span class="hljs-comment">//each oscillator has its own gain for mixing the two waveforms</span>
    <span class="hljs-comment">//the sample outputs would be divided by the number of playing voices to prevent blowing out speakers and deafening user</span>
    <span class="hljs-keyword">float</span> sample1 = (_osc1.get_sample() * _osc1_amp)/NUM_VOICES; 
    <span class="hljs-keyword">float</span> sample2 = (_osc2.get_sample() * _osc2_amp)/NUM_VOICES;
    <span class="hljs-keyword">return</span> (sample1 + sample2) * amp_env.val;
}
</div></code></pre>
<p>Because the envelope does not actually produce any sound, the process method does not need to be called during the audio callback. The hardware timer callback would call the process method and the envelope would change its gain according to its current phase. The configuration of the timer is used to make sure the various time based phases affect the envelope gain correctly according to time (Ex. If attack is set to 100 milliseconds, the gain should increase by .001 every millisecond)</p>
<table>
<thead>
<tr>
<th>Envelope States</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ready</td>
<td>Indicates to synth owning voice that its available to play note</td>
</tr>
<tr>
<td>Attack</td>
<td>Gain increases up to 1.0 according to attack rate</td>
</tr>
<tr>
<td>Decay</td>
<td>Gain decreases to sustain level according to decay rate</td>
</tr>
<tr>
<td>Sustain</td>
<td>Gain remains at level according to sustain</td>
</tr>
<tr>
<td>Release</td>
<td>Gain decreases to 0.0 according to release</td>
</tr>
</tbody>
</table>
<img src="images/envelope_state_diagram.png" alt="Envelope State Diagram" width=400 title="Envelope State Diagram">
<h2 id="filters">Filters</h2>
<p>The last of the core functions to be implemented in the synth was signal filtering. Up until this point, the control of the harmonic qualities of the signals was fairly limited. You could combine different fundamental waveforms and alter pitch to introduce <em>some</em> complexity, but the subtractive aspect of this subtractive synthesizer was conspicuously missing. Filters make up the subtractive nature of the system and allow the user to selectively chop frequency bands from the final signal to emulate real life instruments. An audio filter typically presents two main parameters: Frequency cutoff and resonance. Cutoff determines what frequencies in the spectrum will be filtered out. Resonance is a gain boost that can be applied around the cutoff frequency that will accentuate a certain range of frequencies in the spectrum.</p>
<p>The most common filters used in subtractive synthesis are low pass, high pass and band pass filters. Applying a low pass filter to a harmonically rich signal has the effect of removing the harsher upper harmonic frequencies and make the signal more mellow and deep. This kind of filter is effective in producing sounds like a bass guitar, base drum or cello. Applying a high pass filter will cut out the low frequencies in the signal. This is useful for creating a lead instrument intended to cut through the spectrum without sounding muddy from the lower range of frequencies. Usually a high pass filter will be used to produce sounds like a flute, violin or snare drum. A band pass filter is a combination of low and high pass filters, where a small frequency band somewhere in between is desired.</p>
<p>I wrote a generic filter class that could be subclassed into the different types of filters.</p>
<pre class="hljs"><code><div><span class="hljs-comment">//Filter.h</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Filter</span> {</span>
<span class="hljs-keyword">public</span>:
    Filter() {};
    Filter(<span class="hljs-keyword">float</span> sampleRate, <span class="hljs-keyword">float</span> cutoffFrequency, <span class="hljs-keyword">float</span> q);
    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">float</span> <span class="hljs-title">process</span><span class="hljs-params">(<span class="hljs-keyword">float</span> input)</span> </span>= <span class="hljs-number">0</span>;
    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_cutoff</span><span class="hljs-params">(<span class="hljs-keyword">uint32_t</span> freq_hz)</span></span>;
    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_resonance</span><span class="hljs-params">(<span class="hljs-keyword">uint32_t</span> gain)</span></span>;
    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">void</span> <span class="hljs-title">update_coefs</span><span class="hljs-params">()</span> </span>= <span class="hljs-number">0</span>;

    <span class="hljs-keyword">float</span> cutoffFreq;
    <span class="hljs-keyword">float</span> resonance;
    
<span class="hljs-keyword">protected</span>:
    <span class="hljs-keyword">float</span> sampleRate;
    <span class="hljs-keyword">float</span> b0, b1, b2, a0, a1, a2;
    <span class="hljs-keyword">float</span> x1, x2, y1, y2;
};

<span class="hljs-comment">//Filter.cpp</span>
<span class="hljs-comment">//Subclasses override process and update_coefs functions according to their functions</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LowPassFilter</span> :</span> <span class="hljs-keyword">public</span> Filter;
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HighPassFilter</span> :</span> <span class="hljs-keyword">public</span> Filter;
</div></code></pre>
<p>This part of the synthesizer was the biggest unknown to me at the time of implementation as I needed to learn more about digital signal processing to understand the math. I did a little research, and came across the concept of filtering audio signals using the difference equation. Since my system works by generating audio one sample at a time, it made sense use to filter with discrete time samples. I was able to grasp the concept with a little extra effort. The process of designing a filter using the difference equation is as follows:</p>
<ol>
<li>Based upon your desired filter, you must choose a method of generating appropriate filter coefficients. Some methods include Bilinear Transform, Chebyshev, Finite Impulse Response (FIR), Butterworth etc.</li>
<li>Using your chosen filter method, generate coefficients to be used in your difference equation. You may generate an arbitrary amount of coefficients but it will not necessarily make your filter <strong>better</strong>. It will make it more computationally costly and therefore you must generate an amount of coefficients appropriate to the filter design you choose to implement.</li>
<li>Implement the difference equation to operate on your samples with your generated coefficients.</li>
</ol>
<p>After some research I decided to use the bilinear transform to generate my filter coefficients for my low pass and high pass filters. The filter coefficients are generated performing the bilinear transform on the cutoff frequency divided by the sample rate of the system. The alpha portion of the filter represents how steep the attenuation of the signal is around the cutoff frequency of the filter. Resonance plays a part in the alpha coefficients as the resonance will factor into how much gain is applied around the cutoff frequency of the filter.</p>
<p>Following is my code implementation of the low and high pass filters in my project with a sequence diagram to help visualize the flow of data through the filter and how previously generated values are stored for use on the next filter cycle.</p>
<pre class="hljs"><code><div><span class="hljs-comment">//Filter.cpp</span>
<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">LowPassFilter::update_coefs</span><span class="hljs-params">()</span> </span>{
    <span class="hljs-keyword">float</span> w0 = <span class="hljs-number">2.0f</span> * M_PI * cutoffFreq / sampleRate;
    <span class="hljs-keyword">float</span> cosw0 = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cos</span>(w0);
    <span class="hljs-keyword">float</span> alpha = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">sin</span>(w0) / (<span class="hljs-number">2.0f</span> * resonance);

    b0 = (<span class="hljs-number">1.0f</span> - cosw0) / <span class="hljs-number">2.0f</span>;
    b1 = <span class="hljs-number">1.0f</span> - cosw0;
    b2 = (<span class="hljs-number">1.0f</span> - cosw0) / <span class="hljs-number">2.0f</span>;
    a0 = <span class="hljs-number">1.0f</span> + alpha;
    a1 = <span class="hljs-number">-2.0f</span> * cosw0;
    a2 = <span class="hljs-number">1.0f</span> - alpha;
}

<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">HighPassFilter::update_coefs</span><span class="hljs-params">()</span> </span>{
    <span class="hljs-keyword">float</span> w0 = <span class="hljs-number">2.0f</span> * M_PI * cutoffFreq / sampleRate;
    <span class="hljs-keyword">float</span> alpha = <span class="hljs-built_in">sin</span>(w0) / (<span class="hljs-number">2.0f</span> * resonance);

    b0 = (<span class="hljs-number">1.0f</span> + <span class="hljs-built_in">cos</span>(w0)) / <span class="hljs-number">2.0f</span>;
    b1 = -(<span class="hljs-number">1.0f</span> + <span class="hljs-built_in">cos</span>(w0));
    b2 = (<span class="hljs-number">1.0f</span> + <span class="hljs-built_in">cos</span>(w0)) / <span class="hljs-number">2.0f</span>;
    a0 = <span class="hljs-number">1.0f</span> + alpha;
    a1 = <span class="hljs-number">-2.0f</span> * <span class="hljs-built_in">cos</span>(w0);
    a2 = <span class="hljs-number">1.0f</span> - alpha;
}

<span class="hljs-comment">//The second-order difference equation showing how the output is generated based off previous inputs. The input and output at time of process are preserved and saved to x1 and y1 and the input/ouput values from the previous process cycle are saved to x2 and y2.</span>
<span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">LowPassFilter::process</span><span class="hljs-params">(<span class="hljs-keyword">float</span> input)</span> </span>{
    <span class="hljs-keyword">float</span> output = b0 / a0 * input + b1 / a0 * x1 + b2 / a0 * x2
                     - a1 / a0 * y1 - a2 / a0 * y2;

    x2 = x1;
    x1 = input;
    y2 = y1;
    y1 = output;

    <span class="hljs-keyword">return</span> output;
}
</div></code></pre>
<img src="images/filter_sequence_diagram.png" alt="Filter Sequence Diagram" width=400 title="Filter Sequence Diagram">
<p>Adding filters to the project was a difficult concept to grasp. Some concepts had to be taken with a bit of faith and proven by seeing the in action. This first naive attempt at filtering resulted in some stable low and high pass filters and some gained confidence. I believe now I can move forward and be able to do some deeper research into filters and implement more complex and capable filtering in the future.</p>
<h2 id="architecture-high-level">Architecture High Level</h2>
<p>I only covered a fraction of the overall architecture of the system. The project can be represented in a high level with</p>
<h2 id="the-little-big-software-features">The little big software features</h2>
<p>These are features that I would say are overlooked when they work, but utterly infuriating when they're broken. Some features were so important that they couldn't be ignored but sucked away a critical amount of time from the actual audio processing.</p>
<p><strong>The LCD Screen</strong>
I setup I2C and wrote a driver to interface that allowed me to quickly print menu information to the screen. This was a tediously long process with a lot of trial and error and sucked up about 2-3 weeks of my time. But it works very well and was critical in implementing the menu system.</p>
<p><strong>The Hardware Timer</strong>
I setup a hardware timer that triggered an ISR for reading peripherals. This effectively is a polling mechanism because of the lack of support for GPIO interupts in libDaisy. This timer callback would read the analog potentiometers, the rotary encoder, the MIDI UART line and respond to any inputs on these peripherals. This timer was also used to advance the state of the Amp Envelope (See envelope section for details).</p>
<p><strong>The Menu</strong>
Running with the driver code for the LCD screen, I built up a class that will display the menu and their corresponding contexts. The Menu maintains a structure of configurable parameters for the synthesizer such ass oscillator waveforms and filters. The general purpose potentiometers to adaptively manipulate menu parameters under the hood according to the currently selected menu context.</p>
<p><strong>Remote Control GUI Application</strong>
A simple GUI control panel was written in python using the QT framework to be able to remotely change parameters without the need of physically interacting with the synthesizer beyond simply playing keys on the keyboard to generate sound. This was accomplished using a secondary USB cable to communicate via USB Serial to tell the synth what parameters to change. This GUI also allows the user to save custom patches to a file on the host PC's disk, which can be loaded at will to instantly reconfigure the system. This feature is still in development but mostly complete with the ability to change change a few parameters. The USB Serial driver provided by daisy is succeptable to crashing when too much data is sent down the line to quickly.</p>
<h1 id="project-results">Project Results</h1>
<p>Overall the project was a mixed bag of successes and ongoing struggles to implement.</p>
<h2 id="successes-and-completed-features">Successes and completed features</h2>
<p>I accomplished to core requirement of my project. Implement a real-time subtractive synthesizer from scratch on an embedded device. My system generates tones based on user input from a generic MIDI keyboard. The tones are combined and processed through an amp envelope to add dynamics. The output from the envelope is processed through filters to carve frequencies out of the audio signal. The final processed audio signal is output to speakers or headphones which can be heard by the user in real time without interrupts like pops or clicks in the audio.</p>
<h2 id="failures">Failures</h2>
<p>I did not manage to implement some of my desired audio components to the system. I did not have time to add low frequency oscillators (LFO) which could be used for general purpose frequency modulation for things like pitch and filter cutoff. I did not have time to add support for saving patch parameters to flash on the device itself. Bugs were encountered in my chosen library that made this too difficult to accomplish in my timeline. Many setback were encounter while adding hardware support that cost valuable time. It took several attempts with weeks of effort to create a handmade circuit board to contain all my hardware components. My first attempt at laying all the electrical components on a circuit board was based around making flying wire harnesses that attach to all the human input peripherals. This turned out to be an absolute mess and had to be scrapped. My thought was that by making flying harnesses, this would make it easier to put the whole system into an enclosure. What resulted was a noisy mess of wires that made me feel physically ill. In the process of adding an external power supply capable of providing power to the MIDI input circuit and LCD screen, I accidentally wired my power input wrong to my microcontroller and I fried the device, releasing the magic blue smoke. I had to move forward with development for and adapt to not having a microcontroller while I waited for a new one to be developed.</p>
<h2 id="planned-but-not-completed">Planned but not completed</h2>
<p>I had several features I still wanted to implement in the system. I hoped to add some general purpose Envelopes to the system that the user could route to various components to be used for modulating arbitrary signals like filter cutoff, audio effect gain, oscillator pitch etc. I planned to implement another common and important component in synthesis: the Low Frequency Oscillator (LFO). LFOs are non audible oscillators that are used for arbitrary signal modulation similar to the general purpose envelope mentioned previously. You can take the generated signal of the LFO and plug it into things like the pitch of and oscillator to add pitch vibrator. You can also apply it to filter cutoff to create filter &quot;swishing&quot; to the signal, or apply it to the gain of the left and right audio outputs in order to create volume panning between the left and right channels. These two features would not have been much effort to add and I still intend to implement them. Finally I wanted to add some final audio effects that could process signals at the end of the audio pipeline like reverb, delay, overdrive and phaser. These final effects (fx) can add spatial features and additional audio coloring and complexity that users of a synth enjoy playing with most.</p>
<h2 id="hindsight-choices-were-made">Hindsight: Choices were made</h2>
<ol>
<li>The Daisy Platform: Choosing to base my masters project on the promising platform of Daisy had serious consequences on development. Daisy was a very attractive option that promised a system that would make the hardware aspects get out of the way and make it easier to write audio processing software. The truth is, this is a new technology that has most things implemented to make it easier for hobbiests to get coding and make their dream system. I would have been better off in choosing a more mature audio centric board from ST Microelectronics and using the tried and true STM32Cube IDE to setup my hardware.</li>
<li>Hardware Implementation: Too much time was spent trying to migrate my project off of a breadboard and onto a permanent circuit board. I thought I knew enough of what I was doing that I could make a circuit board that could be the final resting place of my project until I was finished. I tried to setup a prototype with flying harnesses for all the peripherals to make it easy to embed in an enclosure box. What resulted was a noisy mess of wires that made me physically ill to look at. After all that effort, I had to toss that prototype in the garbage. I would have been better off just soldering my components to some perf board and doing some uncomplicated wiring of all my data buses directly to the socket pins of my microcontroller.</li>
<li>Spaghetti Code and Tight Coupling to the Microcontroller: As the system rapidly developed, some holes in previously developed features became more obvious with time which had to be ignored in favor of moving forward with development in order to meet the deadline. At times, refactoring software would create regressions that would cost even more time to fix. Software refactoring and reorganization would definitely help to make it faster and easier to add additional functionality to the system. Looking back, I wish that I had properly planned out making my individual audio components modular enough that I could easily test them out without the microcontroller and the synth system as a whole. If I had made every component independent enough on the system, I could have been writing most of my audio processing software work outside the system and test them by making them perform processing simple audio samples on my computer instead of having to write my code, compile, flash, test, fix and repeat. It would have even been a desireable option to make the synthesizer capable of compiling to the native system of the host computer and run in a minimal settings to be able to test features without the hardware in the loop.</li>
</ol>
<h2 id="performance-issues-and-bottlenecks">Performance Issues and Bottlenecks</h2>
<p>Overall, there doesn't seem to be much of a bottleneck in the audio pipeline within my project. The synth is never late to deliver audio samples to the output DAC, so there are never any audio crackles or pops in the final audio played out the speakers.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In my eyes, this project was a success, but not nearly the success that I envisioned and hoped for. I spent hundreds of hours and countless late nights probing my code for software bugs, probing hardware pins with a multimeter and oscilloscope. Many late nights were spent in the company of a soldering iron and writing tests to check that my hardware was working. Several weeks were spent writing and testing drivers to interfaces with the LCD screens and flash memory storage. But on the other hand, many hours were spent with the exhillirating feeling of adding a new audio processing feature to my system. Sometimes these wins with adding features just came one after another. Then what followed would be a regression requiring hours of hard work to debug and refactor into the system.</p>
<p>In the end though, I got the core of what I hoped for. I have a system that works. I can plug in a midi keyboard and play notes and hear them play out my headphones as I would expect that to sound. I can dive into the menu and adjust the envelope, filters and waveforms and it works the same as commercial synthesizers that I have used over the years. I've learned what I wanted to learn and so much more. I thought I would be getting a greater primer in how audio synthesis works; but in the end I also got a fantastic hands on lesson in implementing <strong>any</strong> complex software or embedded system. I've gained confidence that I could venture into other foreign domains of software development such as video processing, hardware driver development etc and I could feasibly learn the ropes. In implementing this system, perhaps an entire four year undergraduate degree was learned about the best practices in implementing a complex system from the ground up. The failures that happened along the way were so valuable in illustrating the importance of code modularity. These failures further compounded the necessity of not obsessing over adding that extra special features and instead focus on making the system work while paving a path to improvement. Trying to do too much at once results in wasted time and having to do things over again the right way.</p>
<p>This retrospective is almost cathartic in the clarity it gives me on the past year of my efforts. Surprisingly, the next steps I'm inclinded to take are not to finish the project where it stands now. I want to preserve the code from the this project and use it as a reference and redo this whole project from the ground up. I want to take my audio software components and rewrite them to be decoupled from the daisy platform so that they can be tested outside an embedded environment and even used in software applications. I would then reintegrate these components into a new implementation of my synthesizer but on a more mature and robust embedded platform like an STM32. I would then implement all the missing features from this project like the LFO, audio fx components, patch saving etc to complete my final vision. The last step would be to fabricate my own printed circuit board professionally and put the whole thing in a box to show to the world.</p>
<p>I am so pleased by the results and wisdom gained from this project and I can't wait see it through to the end.</p>

</body>
</html>
